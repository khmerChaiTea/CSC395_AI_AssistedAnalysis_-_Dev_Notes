{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0fec93ad",
   "metadata": {},
   "source": [
    "# Print the head of the homelessness data\n",
    "import pandas as pd\n",
    "\n",
    "print(homelessness.head())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c77b6aa6",
   "metadata": {},
   "source": [
    "               region       state  individuals  family_members  state_pop\n",
    "0  East South Central     Alabama       2570.0           864.0    4887681\n",
    "1             Pacific      Alaska       1434.0           582.0     735139\n",
    "2            Mountain     Arizona       7259.0          2606.0    7158024\n",
    "3  West South Central    Arkansas       2280.0           432.0    3009733\n",
    "4             Pacific  California     109008.0         20964.0   39461588 "
   ]
  },
  {
   "cell_type": "raw",
   "id": "3c4fb291",
   "metadata": {},
   "source": [
    "# Print the head of the homelessness data\n",
    "print(homelessness.head())\n",
    "\n",
    "# Print information about homelessness\n",
    "print(homelessness.info())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fbe574f4",
   "metadata": {},
   "source": [
    "               region       state  individuals  family_members  state_pop\n",
    "0  East South Central     Alabama       2570.0           864.0    4887681\n",
    "1             Pacific      Alaska       1434.0           582.0     735139\n",
    "2            Mountain     Arizona       7259.0          2606.0    7158024\n",
    "3  West South Central    Arkansas       2280.0           432.0    3009733\n",
    "4             Pacific  California     109008.0         20964.0   39461588\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "Int64Index: 51 entries, 0 to 50\n",
    "Data columns (total 5 columns):\n",
    " #   Column          Non-Null Count  Dtype  \n",
    "---  ------          --------------  -----  \n",
    " 0   region          51 non-null     object \n",
    " 1   state           51 non-null     object \n",
    " 2   individuals     51 non-null     float64\n",
    " 3   family_members  51 non-null     float64\n",
    " 4   state_pop       51 non-null     int64  \n",
    "dtypes: float64(2), int64(1), object(2)\n",
    "memory usage: 2.4+ KB\n",
    "None"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0b0f66c2",
   "metadata": {},
   "source": [
    "# Print the head of the homelessness data\n",
    "print(homelessness.head())\n",
    "\n",
    "# Print information about homelessness\n",
    "print(homelessness.info())\n",
    "\n",
    "# Print the shape of homelessness\n",
    "print(homelessness.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "322820ae",
   "metadata": {},
   "source": [
    "             region       state  individuals  family_members  state_pop\n",
    "0  East South Central     Alabama       2570.0           864.0    4887681\n",
    "1             Pacific      Alaska       1434.0           582.0     735139\n",
    "2            Mountain     Arizona       7259.0          2606.0    7158024\n",
    "3  West South Central    Arkansas       2280.0           432.0    3009733\n",
    "4             Pacific  California     109008.0         20964.0   39461588\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "Int64Index: 51 entries, 0 to 50\n",
    "Data columns (total 5 columns):\n",
    " #   Column          Non-Null Count  Dtype  \n",
    "---  ------          --------------  -----  \n",
    " 0   region          51 non-null     object \n",
    " 1   state           51 non-null     object \n",
    " 2   individuals     51 non-null     float64\n",
    " 3   family_members  51 non-null     float64\n",
    " 4   state_pop       51 non-null     int64  \n",
    "dtypes: float64(2), int64(1), object(2)\n",
    "memory usage: 2.4+ KB\n",
    "None\n",
    "(51, 5)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a2a5201b",
   "metadata": {},
   "source": [
    "# Print the head of the homelessness data\n",
    "print(homelessness.head())\n",
    "\n",
    "# Print information about homelessness\n",
    "print(homelessness.info())\n",
    "\n",
    "# Print the shape of homelessness\n",
    "print(homelessness.shape)\n",
    "\n",
    "# Print a description of homelessness\n",
    "print(homelessness.describe())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7d010cb0",
   "metadata": {},
   "source": [
    "               region       state  individuals  family_members  state_pop\n",
    "0  East South Central     Alabama       2570.0           864.0    4887681\n",
    "1             Pacific      Alaska       1434.0           582.0     735139\n",
    "2            Mountain     Arizona       7259.0          2606.0    7158024\n",
    "3  West South Central    Arkansas       2280.0           432.0    3009733\n",
    "4             Pacific  California     109008.0         20964.0   39461588\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "Int64Index: 51 entries, 0 to 50\n",
    "Data columns (total 5 columns):\n",
    " #   Column          Non-Null Count  Dtype  \n",
    "---  ------          --------------  -----  \n",
    " 0   region          51 non-null     object \n",
    " 1   state           51 non-null     object \n",
    " 2   individuals     51 non-null     float64\n",
    " 3   family_members  51 non-null     float64\n",
    " 4   state_pop       51 non-null     int64  \n",
    "dtypes: float64(2), int64(1), object(2)\n",
    "memory usage: 2.4+ KB\n",
    "None\n",
    "(51, 5)\n",
    "       individuals  family_members  state_pop\n",
    "count       51.000          51.000  5.100e+01\n",
    "mean      7225.784        3504.882  6.406e+06\n",
    "std      15991.025        7805.412  7.327e+06\n",
    "min        434.000          75.000  5.776e+05\n",
    "25%       1446.500         592.000  1.777e+06\n",
    "50%       3082.000        1482.000  4.461e+06\n",
    "75%       6781.500        3196.000  7.341e+06\n",
    "max     109008.000       52070.000  3.946e+07"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d63f2700",
   "metadata": {},
   "source": [
    "# Import pandas using the alias pd\n",
    "import pandas as pd\n",
    "\n",
    "# Print the values of homelessness\n",
    "print(homelessness.values)\n",
    "\n",
    "# Print the column index of homelessness\n",
    "print(homelessness.columns)\n",
    "# Print the row index of homelessness\n",
    "print(homelessness.index)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f4cfc830",
   "metadata": {},
   "source": [
    "[['East South Central' 'Alabama' 2570.0 864.0 4887681]\n",
    " ['Pacific' 'Alaska' 1434.0 582.0 735139]\n",
    " ['Mountain' 'Arizona' 7259.0 2606.0 7158024]\n",
    " ['West South Central' 'Arkansas' 2280.0 432.0 3009733]\n",
    " ['Pacific' 'California' 109008.0 20964.0 39461588]\n",
    " ['Mountain' 'Colorado' 7607.0 3250.0 5691287]\n",
    " ['New England' 'Connecticut' 2280.0 1696.0 3571520]\n",
    " ['South Atlantic' 'Delaware' 708.0 374.0 965479]\n",
    " ['South Atlantic' 'District of Columbia' 3770.0 3134.0 701547]\n",
    " ['South Atlantic' 'Florida' 21443.0 9587.0 21244317]\n",
    " ['South Atlantic' 'Georgia' 6943.0 2556.0 10511131]\n",
    " ['Pacific' 'Hawaii' 4131.0 2399.0 1420593]\n",
    " ['Mountain' 'Idaho' 1297.0 715.0 1750536]\n",
    " ['East North Central' 'Illinois' 6752.0 3891.0 12723071]\n",
    " ['East North Central' 'Indiana' 3776.0 1482.0 6695497]\n",
    " ['West North Central' 'Iowa' 1711.0 1038.0 3148618]\n",
    " ['West North Central' 'Kansas' 1443.0 773.0 2911359]\n",
    " ['East South Central' 'Kentucky' 2735.0 953.0 4461153]\n",
    " ['West South Central' 'Louisiana' 2540.0 519.0 4659690]\n",
    " ['New England' 'Maine' 1450.0 1066.0 1339057]\n",
    " ['South Atlantic' 'Maryland' 4914.0 2230.0 6035802]\n",
    " ['New England' 'Massachusetts' 6811.0 13257.0 6882635]\n",
    " ['East North Central' 'Michigan' 5209.0 3142.0 9984072]\n",
    " ['West North Central' 'Minnesota' 3993.0 3250.0 5606249]\n",
    " ['East South Central' 'Mississippi' 1024.0 328.0 2981020]\n",
    " ['West North Central' 'Missouri' 3776.0 2107.0 6121623]\n",
    " ['Mountain' 'Montana' 983.0 422.0 1060665]\n",
    " ['West North Central' 'Nebraska' 1745.0 676.0 1925614]\n",
    " ['Mountain' 'Nevada' 7058.0 486.0 3027341]\n",
    " ['New England' 'New Hampshire' 835.0 615.0 1353465]\n",
    " ['Mid-Atlantic' 'New Jersey' 6048.0 3350.0 8886025]\n",
    " ['Mountain' 'New Mexico' 1949.0 602.0 2092741]\n",
    " ['Mid-Atlantic' 'New York' 39827.0 52070.0 19530351]\n",
    " ['South Atlantic' 'North Carolina' 6451.0 2817.0 10381615]\n",
    " ['West North Central' 'North Dakota' 467.0 75.0 758080]\n",
    " ['East North Central' 'Ohio' 6929.0 3320.0 11676341]\n",
    " ['West South Central' 'Oklahoma' 2823.0 1048.0 3940235]\n",
    " ['Pacific' 'Oregon' 11139.0 3337.0 4181886]\n",
    " ['Mid-Atlantic' 'Pennsylvania' 8163.0 5349.0 12800922]\n",
    " ['New England' 'Rhode Island' 747.0 354.0 1058287]\n",
    " ['South Atlantic' 'South Carolina' 3082.0 851.0 5084156]\n",
    " ['West North Central' 'South Dakota' 836.0 323.0 878698]\n",
    " ['East South Central' 'Tennessee' 6139.0 1744.0 6771631]\n",
    " ['West South Central' 'Texas' 19199.0 6111.0 28628666]\n",
    " ['Mountain' 'Utah' 1904.0 972.0 3153550]\n",
    " ['New England' 'Vermont' 780.0 511.0 624358]\n",
    " ['South Atlantic' 'Virginia' 3928.0 2047.0 8501286]\n",
    " ['Pacific' 'Washington' 16424.0 5880.0 7523869]\n",
    " ['South Atlantic' 'West Virginia' 1021.0 222.0 1804291]\n",
    " ['East North Central' 'Wisconsin' 2740.0 2167.0 5807406]\n",
    " ['Mountain' 'Wyoming' 434.0 205.0 577601]]\n",
    "Index(['region', 'state', 'individuals', 'family_members', 'state_pop'], dtype='object')\n",
    "Int64Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48,\n",
    "            49, 50],\n",
    "           dtype='int64')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "09978ee3",
   "metadata": {},
   "source": [
    "# Sort homelessness by individuals\n",
    "homelessness_ind = homelessness.sort_values(\"individuals\")\n",
    "\n",
    "# Print the top few rows\n",
    "print(homelessness_ind.head())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d5209e03",
   "metadata": {},
   "source": [
    "                region         state  individuals  family_members  state_pop\n",
    "50            Mountain       Wyoming        434.0           205.0     577601\n",
    "34  West North Central  North Dakota        467.0            75.0     758080\n",
    "7       South Atlantic      Delaware        708.0           374.0     965479\n",
    "39         New England  Rhode Island        747.0           354.0    1058287\n",
    "45         New England       Vermont        780.0           511.0     624358"
   ]
  },
  {
   "cell_type": "raw",
   "id": "41156433",
   "metadata": {},
   "source": [
    "# Sort homelessness by descending family members\n",
    "homelessness_fam = homelessness.sort_values(by=\"family_members\", ascending=False)\n",
    "\n",
    "# Print the top few rows\n",
    "print(homelessness_fam.head())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8ada1a3a",
   "metadata": {},
   "source": [
    "                region          state  individuals  family_members  state_pop\n",
    "32        Mid-Atlantic       New York      39827.0         52070.0   19530351\n",
    "4              Pacific     California     109008.0         20964.0   39461588\n",
    "21         New England  Massachusetts       6811.0         13257.0    6882635\n",
    "9       South Atlantic        Florida      21443.0          9587.0   21244317\n",
    "43  West South Central          Texas      19199.0          6111.0   28628666"
   ]
  },
  {
   "cell_type": "raw",
   "id": "efc3bd99",
   "metadata": {},
   "source": [
    "# Sort homelessness by region, then descending family members\n",
    "homelessness_reg_fam = homelessness.sort_values(by=['region', 'family_members'], ascending=[True, False])\n",
    "\n",
    "# Print the top few rows\n",
    "print(homelessness_reg_fam.head())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "355c3370",
   "metadata": {},
   "source": [
    "<script.py> output:\n",
    "                    region          state  individuals  family_members  state_pop\n",
    "    32        Mid-Atlantic       New York      39827.0         52070.0   19530351\n",
    "    4              Pacific     California     109008.0         20964.0   39461588\n",
    "    21         New England  Massachusetts       6811.0         13257.0    6882635\n",
    "    9       South Atlantic        Florida      21443.0          9587.0   21244317\n",
    "    43  West South Central          Texas      19199.0          6111.0   28628666\n",
    "\n",
    "<script.py> output:\n",
    "                    region      state  individuals  family_members  state_pop\n",
    "    13  East North Central   Illinois       6752.0          3891.0   12723071\n",
    "    35  East North Central       Ohio       6929.0          3320.0   11676341\n",
    "    22  East North Central   Michigan       5209.0          3142.0    9984072\n",
    "    49  East North Central  Wisconsin       2740.0          2167.0    5807406\n",
    "    14  East North Central    Indiana       3776.0          1482.0    6695497"
   ]
  },
  {
   "cell_type": "raw",
   "id": "82487ea3",
   "metadata": {},
   "source": [
    "# Select the individuals column\n",
    "individuals = homelessness['individuals']\n",
    "\n",
    "# Print the head of the result\n",
    "print(individuals.head())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1899bb5a",
   "metadata": {},
   "source": [
    "0      2570.0\n",
    "1      1434.0\n",
    "2      7259.0\n",
    "3      2280.0\n",
    "4    109008.0\n",
    "Name: individuals, dtype: float64"
   ]
  },
  {
   "cell_type": "raw",
   "id": "abaa932c",
   "metadata": {},
   "source": [
    "# Select the state and family_members columns\n",
    "state_fam = homelessness[['state', 'family_members']]\n",
    "\n",
    "# Print the head of the result\n",
    "print(state_fam.head())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "38951a05",
   "metadata": {},
   "source": [
    "        state  family_members\n",
    "0     Alabama           864.0\n",
    "1      Alaska           582.0\n",
    "2     Arizona          2606.0\n",
    "3    Arkansas           432.0\n",
    "4  California         20964.0"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3282c2a4",
   "metadata": {},
   "source": [
    "# Select only the individuals and state columns, in that order\n",
    "ind_state = homelessness[['individuals', 'state']]\n",
    "\n",
    "# Print the head of the result\n",
    "print(ind_state.head())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "97ae78f9",
   "metadata": {},
   "source": [
    "   individuals       state\n",
    "0       2570.0     Alabama\n",
    "1       1434.0      Alaska\n",
    "2       7259.0     Arizona\n",
    "3       2280.0    Arkansas\n",
    "4     109008.0  California"
   ]
  },
  {
   "cell_type": "raw",
   "id": "29243b6f",
   "metadata": {},
   "source": [
    "# Filter for rows where individuals is greater than 10000\n",
    "ind_gt_10k = homelessness[homelessness['individuals'] > 10000]\n",
    "\n",
    "# See the result\n",
    "print(ind_gt_10k)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bd68a7ec",
   "metadata": {},
   "source": [
    "                region       state  individuals  family_members  state_pop\n",
    "4              Pacific  California     109008.0         20964.0   39461588\n",
    "9       South Atlantic     Florida      21443.0          9587.0   21244317\n",
    "32        Mid-Atlantic    New York      39827.0         52070.0   19530351\n",
    "37             Pacific      Oregon      11139.0          3337.0    4181886\n",
    "43  West South Central       Texas      19199.0          6111.0   28628666\n",
    "47             Pacific  Washington      16424.0          5880.0    7523869"
   ]
  },
  {
   "cell_type": "raw",
   "id": "866f97de",
   "metadata": {},
   "source": [
    "# Filter for rows where region is Mountain\n",
    "mountain_reg = homelessness[homelessness['region'] == 'Mountain']\n",
    "\n",
    "# See the result\n",
    "print(mountain_reg)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "89123781",
   "metadata": {},
   "source": [
    "      region       state  individuals  family_members  state_pop\n",
    "2   Mountain     Arizona       7259.0          2606.0    7158024\n",
    "5   Mountain    Colorado       7607.0          3250.0    5691287\n",
    "12  Mountain       Idaho       1297.0           715.0    1750536\n",
    "26  Mountain     Montana        983.0           422.0    1060665\n",
    "28  Mountain      Nevada       7058.0           486.0    3027341\n",
    "31  Mountain  New Mexico       1949.0           602.0    2092741\n",
    "44  Mountain        Utah       1904.0           972.0    3153550\n",
    "50  Mountain     Wyoming        434.0           205.0     577601"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c7bd0264",
   "metadata": {},
   "source": [
    "# Filter for rows where family_members is less than 1000 \n",
    "# and region is Pacific\n",
    "fam_lt_1k_pac = homelessness[(homelessness['family_members'] < 1000) & (homelessness['region'] == 'Pacific')]\n",
    "\n",
    "# See the result\n",
    "print(fam_lt_1k_pac)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bbbfdc7b",
   "metadata": {},
   "source": [
    "    region   state  individuals  family_members  state_pop\n",
    "1  Pacific  Alaska       1434.0           582.0     735139"
   ]
  },
  {
   "cell_type": "raw",
   "id": "98c4be6b",
   "metadata": {},
   "source": [
    "# Subset for rows in South Atlantic or Mid-Atlantic regions\n",
    "south_mid_atlantic = homelessness[(homelessness['region'] == 'South Atlantic') | (homelessness['region'] == 'Mid-Atlantic')]\n",
    "\n",
    "# See the result\n",
    "print(south_mid_atlantic)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "029efb73",
   "metadata": {},
   "source": [
    "            region                 state  individuals  family_members  state_pop\n",
    "7   South Atlantic              Delaware        708.0           374.0     965479\n",
    "8   South Atlantic  District of Columbia       3770.0          3134.0     701547\n",
    "9   South Atlantic               Florida      21443.0          9587.0   21244317\n",
    "10  South Atlantic               Georgia       6943.0          2556.0   10511131\n",
    "20  South Atlantic              Maryland       4914.0          2230.0    6035802\n",
    "30    Mid-Atlantic            New Jersey       6048.0          3350.0    8886025\n",
    "32    Mid-Atlantic              New York      39827.0         52070.0   19530351\n",
    "33  South Atlantic        North Carolina       6451.0          2817.0   10381615\n",
    "38    Mid-Atlantic          Pennsylvania       8163.0          5349.0   12800922\n",
    "40  South Atlantic        South Carolina       3082.0           851.0    5084156\n",
    "46  South Atlantic              Virginia       3928.0          2047.0    8501286\n",
    "48  South Atlantic         West Virginia       1021.0           222.0    1804291"
   ]
  },
  {
   "cell_type": "raw",
   "id": "16d4ade6",
   "metadata": {},
   "source": [
    "# The Mojave Desert states\n",
    "canu = [\"California\", \"Arizona\", \"Nevada\", \"Utah\"]\n",
    "\n",
    "# Filter for rows in the Mojave Desert states\n",
    "mojave_homelessness = homelessness[homelessness['state'].isin(canu)]\n",
    "\n",
    "# See the result\n",
    "print(mojave_homelessness)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a342ae54",
   "metadata": {},
   "source": [
    "      region       state  individuals  family_members  state_pop\n",
    "2   Mountain     Arizona       7259.0          2606.0    7158024\n",
    "4    Pacific  California     109008.0         20964.0   39461588\n",
    "28  Mountain      Nevada       7058.0           486.0    3027341\n",
    "44  Mountain        Utah       1904.0           972.0    3153550"
   ]
  },
  {
   "cell_type": "raw",
   "id": "42937b3c",
   "metadata": {},
   "source": [
    "# Add total col as sum of individuals and family_members\n",
    "homelessness['total'] = homelessness['individuals'] + homelessness['family_members']\n",
    "\n",
    "# Add p_individuals col as proportion of total that are individuals\n",
    "homelessness['p_individuals'] = homelessness['individuals'] / homelessness['total']\n",
    "\n",
    "# See the result\n",
    "print(homelessness)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bc5edb25",
   "metadata": {},
   "source": [
    "                region                 state  individuals  family_members  state_pop     total  p_individuals\n",
    "0   East South Central               Alabama       2570.0           864.0    4887681    3434.0          0.748\n",
    "1              Pacific                Alaska       1434.0           582.0     735139    2016.0          0.711\n",
    "2             Mountain               Arizona       7259.0          2606.0    7158024    9865.0          0.736\n",
    "3   West South Central              Arkansas       2280.0           432.0    3009733    2712.0          0.841\n",
    "4              Pacific            California     109008.0         20964.0   39461588  129972.0          0.839\n",
    "5             Mountain              Colorado       7607.0          3250.0    5691287   10857.0          0.701\n",
    "6          New England           Connecticut       2280.0          1696.0    3571520    3976.0          0.573\n",
    "7       South Atlantic              Delaware        708.0           374.0     965479    1082.0          0.654\n",
    "8       South Atlantic  District of Columbia       3770.0          3134.0     701547    6904.0          0.546\n",
    "9       South Atlantic               Florida      21443.0          9587.0   21244317   31030.0          0.691\n",
    "10      South Atlantic               Georgia       6943.0          2556.0   10511131    9499.0          0.731\n",
    "11             Pacific                Hawaii       4131.0          2399.0    1420593    6530.0          0.633\n",
    "12            Mountain                 Idaho       1297.0           715.0    1750536    2012.0          0.645\n",
    "13  East North Central              Illinois       6752.0          3891.0   12723071   10643.0          0.634\n",
    "14  East North Central               Indiana       3776.0          1482.0    6695497    5258.0          0.718\n",
    "15  West North Central                  Iowa       1711.0          1038.0    3148618    2749.0          0.622\n",
    "16  West North Central                Kansas       1443.0           773.0    2911359    2216.0          0.651\n",
    "17  East South Central              Kentucky       2735.0           953.0    4461153    3688.0          0.742\n",
    "18  West South Central             Louisiana       2540.0           519.0    4659690    3059.0          0.830\n",
    "19         New England                 Maine       1450.0          1066.0    1339057    2516.0          0.576\n",
    "20      South Atlantic              Maryland       4914.0          2230.0    6035802    7144.0          0.688\n",
    "21         New England         Massachusetts       6811.0         13257.0    6882635   20068.0          0.339\n",
    "22  East North Central              Michigan       5209.0          3142.0    9984072    8351.0          0.624\n",
    "23  West North Central             Minnesota       3993.0          3250.0    5606249    7243.0          0.551\n",
    "24  East South Central           Mississippi       1024.0           328.0    2981020    1352.0          0.757\n",
    "25  West North Central              Missouri       3776.0          2107.0    6121623    5883.0          0.642\n",
    "26            Mountain               Montana        983.0           422.0    1060665    1405.0          0.700\n",
    "27  West North Central              Nebraska       1745.0           676.0    1925614    2421.0          0.721\n",
    "28            Mountain                Nevada       7058.0           486.0    3027341    7544.0          0.936\n",
    "29         New England         New Hampshire        835.0           615.0    1353465    1450.0          0.576\n",
    "30        Mid-Atlantic            New Jersey       6048.0          3350.0    8886025    9398.0          0.644\n",
    "31            Mountain            New Mexico       1949.0           602.0    2092741    2551.0          0.764\n",
    "32        Mid-Atlantic              New York      39827.0         52070.0   19530351   91897.0          0.433\n",
    "33      South Atlantic        North Carolina       6451.0          2817.0   10381615    9268.0          0.696\n",
    "34  West North Central          North Dakota        467.0            75.0     758080     542.0          0.862\n",
    "35  East North Central                  Ohio       6929.0          3320.0   11676341   10249.0          0.676\n",
    "36  West South Central              Oklahoma       2823.0          1048.0    3940235    3871.0          0.729\n",
    "37             Pacific                Oregon      11139.0          3337.0    4181886   14476.0          0.769\n",
    "38        Mid-Atlantic          Pennsylvania       8163.0          5349.0   12800922   13512.0          0.604\n",
    "39         New England          Rhode Island        747.0           354.0    1058287    1101.0          0.678\n",
    "40      South Atlantic        South Carolina       3082.0           851.0    5084156    3933.0          0.784\n",
    "41  West North Central          South Dakota        836.0           323.0     878698    1159.0          0.721\n",
    "42  East South Central             Tennessee       6139.0          1744.0    6771631    7883.0          0.779\n",
    "43  West South Central                 Texas      19199.0          6111.0   28628666   25310.0          0.759\n",
    "44            Mountain                  Utah       1904.0           972.0    3153550    2876.0          0.662\n",
    "45         New England               Vermont        780.0           511.0     624358    1291.0          0.604\n",
    "46      South Atlantic              Virginia       3928.0          2047.0    8501286    5975.0          0.657\n",
    "47             Pacific            Washington      16424.0          5880.0    7523869   22304.0          0.736\n",
    "48      South Atlantic         West Virginia       1021.0           222.0    1804291    1243.0          0.821\n",
    "49  East North Central             Wisconsin       2740.0          2167.0    5807406    4907.0          0.558\n",
    "50            Mountain               Wyoming        434.0           205.0     577601     639.0          0.679"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8d4b6c52",
   "metadata": {},
   "source": [
    "# Create indiv_per_10k col as homeless individuals per 10k state pop\n",
    "homelessness[\"indiv_per_10k\"] = 10000 * homelessness[\"individuals\"] / homelessness[\"state_pop\"]\n",
    "\n",
    "# Subset rows for indiv_per_10k greater than 20\n",
    "high_homelessness = homelessness[homelessness[\"indiv_per_10k\"] > 20]\n",
    "\n",
    "# Sort high_homelessness by descending indiv_per_10k\n",
    "high_homelessness_srt = high_homelessness.sort_values(by='indiv_per_10k', ascending=False)\n",
    "\n",
    "# From high_homelessness_srt, select the state and indiv_per_10k cols\n",
    "result = high_homelessness_srt[['state', 'indiv_per_10k']]\n",
    "\n",
    "# See the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "73f5c91c",
   "metadata": {},
   "source": [
    "                   state  indiv_per_10k\n",
    "8   District of Columbia         53.738\n",
    "11                Hawaii         29.079\n",
    "4             California         27.624\n",
    "37                Oregon         26.636\n",
    "28                Nevada         23.314\n",
    "47            Washington         21.829\n",
    "32              New York         20.392"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ad7132f2",
   "metadata": {},
   "source": [
    "# Print the head of the sales DataFrame\n",
    "print(sales.head())\n",
    "\n",
    "# Print the info about the sales DataFrame\n",
    "print(sales.info())\n",
    "\n",
    "# Print the mean of weekly_sales\n",
    "print(sales['weekly_sales'].mean())\n",
    "\n",
    "# Print the median of weekly_sales\n",
    "print(sales['weekly_sales'].median())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "71ed2720",
   "metadata": {},
   "source": [
    "   store type  department       date  weekly_sales  is_holiday  temperature_c  fuel_price_usd_per_l  unemployment\n",
    "0      1    A           1 2010-02-05      24924.50       False          5.728                 0.679         8.106\n",
    "1      1    A           1 2010-03-05      21827.90       False          8.056                 0.693         8.106\n",
    "2      1    A           1 2010-04-02      57258.43       False         16.817                 0.718         7.808\n",
    "3      1    A           1 2010-05-07      17413.94       False         22.528                 0.749         7.808\n",
    "4      1    A           1 2010-06-04      17558.09       False         27.050                 0.715         7.808\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 10774 entries, 0 to 10773\n",
    "Data columns (total 9 columns):\n",
    " #   Column                Non-Null Count  Dtype         \n",
    "---  ------                --------------  -----         \n",
    " 0   store                 10774 non-null  int64         \n",
    " 1   type                  10774 non-null  object        \n",
    " 2   department            10774 non-null  int32         \n",
    " 3   date                  10774 non-null  datetime64[ns]\n",
    " 4   weekly_sales          10774 non-null  float64       \n",
    " 5   is_holiday            10774 non-null  bool          \n",
    " 6   temperature_c         10774 non-null  float64       \n",
    " 7   fuel_price_usd_per_l  10774 non-null  float64       \n",
    " 8   unemployment          10774 non-null  float64       \n",
    "dtypes: bool(1), datetime64[ns](1), float64(4), int32(1), int64(1), object(1)\n",
    "memory usage: 641.9+ KB\n",
    "None\n",
    "23843.95014850566\n",
    "12049.064999999999"
   ]
  },
  {
   "cell_type": "raw",
   "id": "198fcdc5",
   "metadata": {},
   "source": [
    "# Print the maximum of the date column\n",
    "print(sales['date'].max())\n",
    "\n",
    "# Print the minimum of the date column\n",
    "print(sales['date'].min())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "37e3cc72",
   "metadata": {},
   "source": [
    "2012-10-26 00:00:00\n",
    "2010-02-05 00:00:00"
   ]
  },
  {
   "cell_type": "raw",
   "id": "abee8dd7",
   "metadata": {},
   "source": [
    "# A custom IQR function\n",
    "def iqr(column):\n",
    "    return column.quantile(0.75) - column.quantile(0.25)\n",
    "    \n",
    "# Print IQR of the temperature_c column\n",
    "print(sales[\"temperature_c\"].agg(iqr))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f0c23c1f",
   "metadata": {},
   "source": [
    "16.583333333333336"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b67b9996",
   "metadata": {},
   "source": [
    "# A custom IQR function\n",
    "def iqr(column):\n",
    "    return column.quantile(0.75) - column.quantile(0.25)\n",
    "\n",
    "# Update to print IQR of temperature_c, fuel_price_usd_per_l, & unemployment\n",
    "print(sales[[\"temperature_c\", \"fuel_price_usd_per_l\", \"unemployment\"]].agg(iqr))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f86af559",
   "metadata": {},
   "source": [
    "temperature_c           16.583\n",
    "fuel_price_usd_per_l     0.073\n",
    "unemployment             0.565\n",
    "dtype: float64"
   ]
  },
  {
   "cell_type": "raw",
   "id": "acf8c95e",
   "metadata": {},
   "source": [
    "# Import NumPy and create custom IQR function\n",
    "import numpy as np\n",
    "def iqr(column):\n",
    "    return column.quantile(0.75) - column.quantile(0.25)\n",
    "\n",
    "# Update to print IQR and median of temperature_c, fuel_price_usd_per_l, & unemployment\n",
    "print(sales[[\"temperature_c\", \"fuel_price_usd_per_l\", \"unemployment\"]].agg([iqr, np.median]))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "24525362",
   "metadata": {},
   "source": [
    "        temperature_c  fuel_price_usd_per_l  unemployment\n",
    "iqr            16.583                 0.073         0.565\n",
    "median         16.967                 0.743         8.099"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1ef2502a",
   "metadata": {},
   "source": [
    "# Sort sales_1_1 by date\n",
    "sales_1_1 = sales_1_1.sort_values(by='date')\n",
    "\n",
    "# Get the cumulative sum of weekly_sales, add as cum_weekly_sales col\n",
    "sales_1_1['cum_weekly_sales'] = sales_1_1['weekly_sales'].cumsum()\n",
    "\n",
    "# Get the cumulative max of weekly_sales, add as cum_max_sales col\n",
    "sales_1_1['cum_max_sales'] = sales_1_1['weekly_sales'].cummax()\n",
    "\n",
    "# See the columns you calculated\n",
    "print(sales_1_1[[\"date\", \"weekly_sales\", \"cum_weekly_sales\", \"cum_max_sales\"]])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "99564b14",
   "metadata": {},
   "source": [
    "<script.py> output:\n",
    "             date  weekly_sales  cum_weekly_sales  cum_max_sales\n",
    "    0  2010-02-05      24924.50          24924.50       24924.50\n",
    "    1  2010-03-05      21827.90          46752.40       24924.50\n",
    "    2  2010-04-02      57258.43         104010.83       57258.43\n",
    "    3  2010-05-07      17413.94         121424.77       57258.43\n",
    "    4  2010-06-04      17558.09         138982.86       57258.43\n",
    "    5  2010-07-02      16333.14         155316.00       57258.43\n",
    "    6  2010-08-06      17508.41         172824.41       57258.43\n",
    "    7  2010-09-03      16241.78         189066.19       57258.43\n",
    "    8  2010-10-01      20094.19         209160.38       57258.43\n",
    "    9  2010-11-05      34238.88         243399.26       57258.43\n",
    "    10 2010-12-03      22517.56         265916.82       57258.43\n",
    "    11 2011-01-07      15984.24         281901.06       57258.43"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ac700173",
   "metadata": {},
   "source": [
    "# Drop duplicate store/type combinations\n",
    "store_types = sales.drop_duplicates(subset=['store', 'type'])\n",
    "print(store_types.head())\n",
    "\n",
    "# Drop duplicate store/department combinations\n",
    "store_depts = sales.drop_duplicates(subset=['store', 'department'])\n",
    "print(store_depts.head())\n",
    "\n",
    "# Subset the rows where is_holiday is True and drop duplicate dates\n",
    "holiday_dates = sales[sales['is_holiday'] == True].drop_duplicates(subset=['date'])\n",
    "\n",
    "# Print date col of holiday_dates\n",
    "print(holiday_dates['date'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7606841a",
   "metadata": {},
   "source": [
    "      store type  department       date  weekly_sales  is_holiday  temperature_c  fuel_price_usd_per_l  unemployment\n",
    "0         1    A           1 2010-02-05      24924.50       False          5.728                 0.679         8.106\n",
    "901       2    A           1 2010-02-05      35034.06       False          4.550                 0.679         8.324\n",
    "1798      4    A           1 2010-02-05      38724.42       False          6.533                 0.686         8.623\n",
    "2699      6    A           1 2010-02-05      25619.00       False          4.683                 0.679         7.259\n",
    "3593     10    B           1 2010-02-05      40212.84       False         12.411                 0.782         9.765\n",
    "    store type  department       date  weekly_sales  is_holiday  temperature_c  fuel_price_usd_per_l  unemployment\n",
    "0       1    A           1 2010-02-05      24924.50       False          5.728                 0.679         8.106\n",
    "12      1    A           2 2010-02-05      50605.27       False          5.728                 0.679         8.106\n",
    "24      1    A           3 2010-02-05      13740.12       False          5.728                 0.679         8.106\n",
    "36      1    A           4 2010-02-05      39954.04       False          5.728                 0.679         8.106\n",
    "48      1    A           5 2010-02-05      32229.38       False          5.728                 0.679         8.106\n",
    "498    2010-09-10\n",
    "691    2011-11-25\n",
    "2315   2010-02-12\n",
    "6735   2012-09-07\n",
    "6810   2010-12-31\n",
    "6815   2012-02-10\n",
    "6820   2011-09-09\n",
    "Name: date, dtype: datetime64[ns]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7c35e9f5",
   "metadata": {},
   "source": [
    "# Count the number of stores of each type\n",
    "store_counts = store_types['type'].value_counts()\n",
    "print(store_counts)\n",
    "\n",
    "# Get the proportion of stores of each type\n",
    "store_props = store_types['type'].value_counts(normalize=True)\n",
    "print(store_props)\n",
    "\n",
    "# Count the number of each department number and sort\n",
    "dept_counts_sorted = store_depts['department'].value_counts(sort=True)\n",
    "print(dept_counts_sorted)\n",
    "\n",
    "# Get the proportion of departments of each number and sort\n",
    "dept_props_sorted = store_depts['department'].value_counts(sort=True, normalize=True)\n",
    "print(dept_props_sorted)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0a4ab5d0",
   "metadata": {},
   "source": [
    "A    11\n",
    "B     1\n",
    "Name: type, dtype: int64\n",
    "A    0.917\n",
    "B    0.083\n",
    "Name: type, dtype: float64\n",
    "1     12\n",
    "55    12\n",
    "72    12\n",
    "71    12\n",
    "67    12\n",
    "      ..\n",
    "37    10\n",
    "48     8\n",
    "50     6\n",
    "39     4\n",
    "43     2\n",
    "Name: department, Length: 80, dtype: int64\n",
    "1     0.013\n",
    "55    0.013\n",
    "72    0.013\n",
    "71    0.013\n",
    "67    0.013\n",
    "      ...  \n",
    "37    0.011\n",
    "48    0.009\n",
    "50    0.006\n",
    "39    0.004\n",
    "43    0.002\n",
    "Name: department, Length: 80, dtype: float64"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d86126ae",
   "metadata": {},
   "source": [
    "# Calc total weekly sales\n",
    "sales_all = sales['weekly_sales'].sum()\n",
    "\n",
    "# Subset for type A stores, calc total weekly sales\n",
    "sales_A = sales[sales['type'] == 'A']['weekly_sales'].sum()\n",
    "\n",
    "# Subset for type B stores, calc total weekly sales\n",
    "sales_B = sales[sales['type'] == 'B']['weekly_sales'].sum()\n",
    "\n",
    "# Subset for type C stores, calc total weekly sales\n",
    "sales_C = sales[sales['type'] == 'C']['weekly_sales'].sum()\n",
    "\n",
    "# Get proportion for each type\n",
    "sales_propn_by_type = [sales_A, sales_B, sales_C] / sales_all\n",
    "print(sales_propn_by_type)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2309c280",
   "metadata": {},
   "source": [
    "[0.9097747 0.0902253 0.       ]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c13dec21",
   "metadata": {},
   "source": [
    "# Group by type; calc total weekly sales\n",
    "sales_by_type = sales.groupby('type')['weekly_sales'].sum()\n",
    "\n",
    "# Get proportion for each type\n",
    "sales_propn_by_type = sales_by_type / sales_by_type.sum()\n",
    "print(sales_propn_by_type)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1bdc9de5",
   "metadata": {},
   "source": [
    "type\n",
    "A    0.91\n",
    "B    0.09\n",
    "Name: weekly_sales, dtype: float64"
   ]
  },
  {
   "cell_type": "raw",
   "id": "94fff7d0",
   "metadata": {},
   "source": [
    "# From previous step\n",
    "sales_by_type = sales.groupby(\"type\")[\"weekly_sales\"].sum()\n",
    "\n",
    "# Group by type and is_holiday; calc total weekly sales\n",
    "sales_by_type_is_holiday = sales.groupby(['type', 'is_holiday'])['weekly_sales'].sum()\n",
    "print(sales_by_type_is_holiday)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "475e8196",
   "metadata": {},
   "source": [
    "type  is_holiday\n",
    "A     False         2.337e+08\n",
    "      True          2.360e+04\n",
    "B     False         2.318e+07\n",
    "      True          1.621e+03\n",
    "Name: weekly_sales, dtype: float64"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4153b4d7",
   "metadata": {},
   "source": [
    "# Import numpy with the alias np\n",
    "import numpy as np\n",
    "\n",
    "# For each store type, aggregate weekly_sales: get min, max, mean, and median\n",
    "sales_stats = sales.groupby('type')['weekly_sales'].agg([np.min, np.max, np.mean, np.median])\n",
    "\n",
    "# Print sales_stats\n",
    "print(sales_stats)\n",
    "\n",
    "# For each store type, aggregate unemployment and fuel_price_usd_per_l: get min, max, mean, and median\n",
    "unemp_fuel_stats = sales.groupby('type')[['unemployment', 'fuel_price_usd_per_l']].agg([np.min, np.max, np.mean, np.median])\n",
    "\n",
    "# Print unemp_fuel_stats\n",
    "print(unemp_fuel_stats)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1272d30f",
   "metadata": {},
   "source": [
    "        amin       amax       mean    median\n",
    "type                                        \n",
    "A    -1098.0  293966.05  23674.667  11943.92\n",
    "B     -798.0  232558.51  25696.678  13336.08\n",
    "     unemployment                      fuel_price_usd_per_l                     \n",
    "             amin   amax   mean median                 amin   amax   mean median\n",
    "type                                                                            \n",
    "A           3.879  8.992  7.973  8.067                0.664  1.107  0.745  0.735\n",
    "B           7.170  9.765  9.279  9.199                0.760  1.108  0.806  0.803"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b5272686",
   "metadata": {},
   "source": [
    "# Pivot for mean weekly_sales for each store type\n",
    "mean_sales_by_type = sales.pivot_table(values='weekly_sales', index='type')\n",
    "\n",
    "# Print mean_sales_by_type\n",
    "print(mean_sales_by_type)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9aa27fd6",
   "metadata": {},
   "source": [
    "      weekly_sales\n",
    "type              \n",
    "A        23674.667\n",
    "B        25696.678"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fc1c07c0",
   "metadata": {},
   "source": [
    "# Import NumPy as np\n",
    "import numpy as np\n",
    "\n",
    "# Pivot for mean and median weekly_sales for each store type\n",
    "mean_med_sales_by_type = sales.pivot_table(values='weekly_sales', index='type', aggfunc=[np.mean, np.median])\n",
    "\n",
    "# Print mean_med_sales_by_type\n",
    "print(mean_med_sales_by_type)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bf02782d",
   "metadata": {},
   "source": [
    "             mean       median\n",
    "     weekly_sales weekly_sales\n",
    "type                          \n",
    "A       23674.667     11943.92\n",
    "B       25696.678     13336.08"
   ]
  },
  {
   "cell_type": "raw",
   "id": "227630f8",
   "metadata": {},
   "source": [
    "# Pivot for mean weekly_sales by store type and holiday \n",
    "mean_sales_by_type_holiday = sales.pivot_table(values='weekly_sales', index='type', columns='is_holiday', aggfunc='mean')\n",
    "\n",
    "# Print mean_sales_by_type_holiday\n",
    "print(mean_sales_by_type_holiday)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4a24b4b9",
   "metadata": {},
   "source": [
    "is_holiday      False     True\n",
    "type                          \n",
    "A           23768.584  590.045\n",
    "B           25751.981  810.705"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9acd7be3",
   "metadata": {},
   "source": [
    "# Print mean weekly_sales by department and type; fill missing values with 0\n",
    "print(sales.pivot_table(values='weekly_sales', index='department', columns='type', aggfunc='mean', fill_value=0))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "de4577e8",
   "metadata": {},
   "source": [
    "type                 A           B\n",
    "department                        \n",
    "1            30961.725   44050.627\n",
    "2            67600.159  112958.527\n",
    "3            17160.003   30580.655\n",
    "4            44285.399   51219.654\n",
    "5            34821.011   63236.875\n",
    "...                ...         ...\n",
    "95          123933.787   77082.102\n",
    "96           21367.043    9528.538\n",
    "97           28471.267    5828.873\n",
    "98           12875.423     217.428\n",
    "99             379.124       0.000\n",
    "\n",
    "[80 rows x 2 columns]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aaaf046d",
   "metadata": {},
   "source": [
    "# Print the mean weekly_sales by department and type; fill missing values with 0s; sum all rows and cols\n",
    "print(sales.pivot_table(values='weekly_sales', index='department', columns='type', aggfunc='mean', fill_value=0, margins=True))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "203623ee",
   "metadata": {},
   "source": [
    "type                A           B        All\n",
    "department                                  \n",
    "1           30961.725   44050.627  32052.467\n",
    "2           67600.159  112958.527  71380.023\n",
    "3           17160.003   30580.655  18278.391\n",
    "4           44285.399   51219.654  44863.254\n",
    "5           34821.011   63236.875  37189.000\n",
    "...               ...         ...        ...\n",
    "96          21367.043    9528.538  20337.608\n",
    "97          28471.267    5828.873  26584.401\n",
    "98          12875.423     217.428  11820.590\n",
    "99            379.124       0.000    379.124\n",
    "All         23674.667   25696.678  23843.950\n",
    "\n",
    "[81 rows x 3 columns]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "034b48a3",
   "metadata": {},
   "source": [
    "# Look at temperatures\n",
    "print(temperatures)\n",
    "\n",
    "# Set the index of temperatures to city\n",
    "temperatures_ind = temperatures.set_index('city')\n",
    "\n",
    "# Look at temperatures_ind\n",
    "print(temperatures_ind)\n",
    "\n",
    "# Reset the temperatures_ind index, keeping its contents\n",
    "print(temperatures_ind.reset_index())\n",
    "\n",
    "# Reset the temperatures_ind index, dropping its contents\n",
    "print(temperatures_ind.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "23f2724e",
   "metadata": {},
   "source": [
    "            date     city        country  avg_temp_c\n",
    "0     2000-01-01  Abidjan  Côte D'Ivoire      27.293\n",
    "1     2000-02-01  Abidjan  Côte D'Ivoire      27.685\n",
    "2     2000-03-01  Abidjan  Côte D'Ivoire      29.061\n",
    "3     2000-04-01  Abidjan  Côte D'Ivoire      28.162\n",
    "4     2000-05-01  Abidjan  Côte D'Ivoire      27.547\n",
    "...          ...      ...            ...         ...\n",
    "16495 2013-05-01     Xian          China      18.979\n",
    "16496 2013-06-01     Xian          China      23.522\n",
    "16497 2013-07-01     Xian          China      25.251\n",
    "16498 2013-08-01     Xian          China      24.528\n",
    "16499 2013-09-01     Xian          China         NaN\n",
    "\n",
    "[16500 rows x 4 columns]\n",
    "              date        country  avg_temp_c\n",
    "city                                         \n",
    "Abidjan 2000-01-01  Côte D'Ivoire      27.293\n",
    "Abidjan 2000-02-01  Côte D'Ivoire      27.685\n",
    "Abidjan 2000-03-01  Côte D'Ivoire      29.061\n",
    "Abidjan 2000-04-01  Côte D'Ivoire      28.162\n",
    "Abidjan 2000-05-01  Côte D'Ivoire      27.547\n",
    "...            ...            ...         ...\n",
    "Xian    2013-05-01          China      18.979\n",
    "Xian    2013-06-01          China      23.522\n",
    "Xian    2013-07-01          China      25.251\n",
    "Xian    2013-08-01          China      24.528\n",
    "Xian    2013-09-01          China         NaN\n",
    "\n",
    "[16500 rows x 3 columns]\n",
    "          city       date        country  avg_temp_c\n",
    "0      Abidjan 2000-01-01  Côte D'Ivoire      27.293\n",
    "1      Abidjan 2000-02-01  Côte D'Ivoire      27.685\n",
    "2      Abidjan 2000-03-01  Côte D'Ivoire      29.061\n",
    "3      Abidjan 2000-04-01  Côte D'Ivoire      28.162\n",
    "4      Abidjan 2000-05-01  Côte D'Ivoire      27.547\n",
    "...        ...        ...            ...         ...\n",
    "16495     Xian 2013-05-01          China      18.979\n",
    "16496     Xian 2013-06-01          China      23.522\n",
    "16497     Xian 2013-07-01          China      25.251\n",
    "16498     Xian 2013-08-01          China      24.528\n",
    "16499     Xian 2013-09-01          China         NaN\n",
    "\n",
    "[16500 rows x 4 columns]\n",
    "            date        country  avg_temp_c\n",
    "0     2000-01-01  Côte D'Ivoire      27.293\n",
    "1     2000-02-01  Côte D'Ivoire      27.685\n",
    "2     2000-03-01  Côte D'Ivoire      29.061\n",
    "3     2000-04-01  Côte D'Ivoire      28.162\n",
    "4     2000-05-01  Côte D'Ivoire      27.547\n",
    "...          ...            ...         ...\n",
    "16495 2013-05-01          China      18.979\n",
    "16496 2013-06-01          China      23.522\n",
    "16497 2013-07-01          China      25.251\n",
    "16498 2013-08-01          China      24.528\n",
    "16499 2013-09-01          China         NaN\n",
    "\n",
    "[16500 rows x 3 columns]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "200a11db",
   "metadata": {},
   "source": [
    "# Make a list of cities to subset on\n",
    "cities = [\"Moscow\", \"Saint Petersburg\"]\n",
    "\n",
    "# Subset temperatures using square brackets\n",
    "print(temperatures[temperatures['city'].isin(cities)])\n",
    "\n",
    "# Subset temperatures_ind using .loc[]\n",
    "print(temperatures_ind.loc[cities])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4401d94f",
   "metadata": {},
   "source": [
    "            date              city country  avg_temp_c\n",
    "10725 2000-01-01            Moscow  Russia      -7.313\n",
    "10726 2000-02-01            Moscow  Russia      -3.551\n",
    "10727 2000-03-01            Moscow  Russia      -1.661\n",
    "10728 2000-04-01            Moscow  Russia      10.096\n",
    "10729 2000-05-01            Moscow  Russia      10.357\n",
    "...          ...               ...     ...         ...\n",
    "13360 2013-05-01  Saint Petersburg  Russia      12.355\n",
    "13361 2013-06-01  Saint Petersburg  Russia      17.185\n",
    "13362 2013-07-01  Saint Petersburg  Russia      17.234\n",
    "13363 2013-08-01  Saint Petersburg  Russia      17.153\n",
    "13364 2013-09-01  Saint Petersburg  Russia         NaN\n",
    "\n",
    "[330 rows x 4 columns]\n",
    "                       date country  avg_temp_c\n",
    "city                                           \n",
    "Moscow           2000-01-01  Russia      -7.313\n",
    "Moscow           2000-02-01  Russia      -3.551\n",
    "Moscow           2000-03-01  Russia      -1.661\n",
    "Moscow           2000-04-01  Russia      10.096\n",
    "Moscow           2000-05-01  Russia      10.357\n",
    "...                     ...     ...         ...\n",
    "Saint Petersburg 2013-05-01  Russia      12.355\n",
    "Saint Petersburg 2013-06-01  Russia      17.185\n",
    "Saint Petersburg 2013-07-01  Russia      17.234\n",
    "Saint Petersburg 2013-08-01  Russia      17.153\n",
    "Saint Petersburg 2013-09-01  Russia         NaN\n",
    "\n",
    "[330 rows x 3 columns]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9ac14bf2",
   "metadata": {},
   "source": [
    "# Index temperatures by country & city\n",
    "temperatures_ind = temperatures.set_index(['country', 'city'])\n",
    "\n",
    "# List of tuples: Brazil, Rio De Janeiro & Pakistan, Lahore\n",
    "rows_to_keep = [(\"Brazil\", \"Rio De Janeiro\"), (\"Pakistan\", \"Lahore\")]\n",
    "\n",
    "# Subset for rows to keep\n",
    "print(temperatures_ind.loc[rows_to_keep]\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1ed9dbae",
   "metadata": {},
   "source": [
    "                              date  avg_temp_c\n",
    "country  city                                 \n",
    "Brazil   Rio De Janeiro 2000-01-01      25.974\n",
    "         Rio De Janeiro 2000-02-01      26.699\n",
    "         Rio De Janeiro 2000-03-01      26.270\n",
    "         Rio De Janeiro 2000-04-01      25.750\n",
    "         Rio De Janeiro 2000-05-01      24.356\n",
    "...                            ...         ...\n",
    "Pakistan Lahore         2013-05-01      33.457\n",
    "         Lahore         2013-06-01      34.456\n",
    "         Lahore         2013-07-01      33.279\n",
    "         Lahore         2013-08-01      31.511\n",
    "         Lahore         2013-09-01         NaN\n",
    "\n",
    "[330 rows x 2 columns]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3a18c16c",
   "metadata": {},
   "source": [
    "# Sort temperatures_ind by index values\n",
    "print(temperatures_ind.sort_index())\n",
    "\n",
    "# Sort temperatures_ind by index values at the city level\n",
    "print(temperatures_ind.sort_index(level='city'))\n",
    "\n",
    "# Sort temperatures_ind by country then descending city\n",
    "print(temperatures_ind.sort_index(level=['country', 'city'], ascending=[True, False]))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7f360405",
   "metadata": {},
   "source": [
    "                         date  avg_temp_c\n",
    "country     city                         \n",
    "Afghanistan Kabul  2000-01-01       3.326\n",
    "            Kabul  2000-02-01       3.454\n",
    "            Kabul  2000-03-01       9.612\n",
    "            Kabul  2000-04-01      17.925\n",
    "            Kabul  2000-05-01      24.658\n",
    "...                       ...         ...\n",
    "Zimbabwe    Harare 2013-05-01      18.298\n",
    "            Harare 2013-06-01      17.020\n",
    "            Harare 2013-07-01      16.299\n",
    "            Harare 2013-08-01      19.232\n",
    "            Harare 2013-09-01         NaN\n",
    "\n",
    "[16500 rows x 2 columns]\n",
    "                            date  avg_temp_c\n",
    "country       city                          \n",
    "Côte D'Ivoire Abidjan 2000-01-01      27.293\n",
    "              Abidjan 2000-02-01      27.685\n",
    "              Abidjan 2000-03-01      29.061\n",
    "              Abidjan 2000-04-01      28.162\n",
    "              Abidjan 2000-05-01      27.547\n",
    "...                          ...         ...\n",
    "China         Xian    2013-05-01      18.979\n",
    "              Xian    2013-06-01      23.522\n",
    "              Xian    2013-07-01      25.251\n",
    "              Xian    2013-08-01      24.528\n",
    "              Xian    2013-09-01         NaN\n",
    "\n",
    "[16500 rows x 2 columns]\n",
    "                         date  avg_temp_c\n",
    "country     city                         \n",
    "Afghanistan Kabul  2000-01-01       3.326\n",
    "            Kabul  2000-02-01       3.454\n",
    "            Kabul  2000-03-01       9.612\n",
    "            Kabul  2000-04-01      17.925\n",
    "            Kabul  2000-05-01      24.658\n",
    "...                       ...         ...\n",
    "Zimbabwe    Harare 2013-05-01      18.298\n",
    "            Harare 2013-06-01      17.020\n",
    "            Harare 2013-07-01      16.299\n",
    "            Harare 2013-08-01      19.232\n",
    "            Harare 2013-09-01         NaN\n",
    "\n",
    "[16500 rows x 2 columns]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "478eab2b",
   "metadata": {},
   "source": [
    "# Sort the index of temperatures_ind\n",
    "temperatures_srt = temperatures_ind.sort_index()\n",
    "\n",
    "# Subset rows from Pakistan to Russia\n",
    "print(temperatures_srt.loc[\"Pakistan\":\"Russia\"])\n",
    "\n",
    "# Try to subset rows from Lahore to Moscow\n",
    "print(temperatures_srt.loc[\"Lahore\":\"Moscow\"])\n",
    "\n",
    "# Subset rows from Pakistan, Lahore to Russia, Moscow\n",
    "print(temperatures_srt.loc[(\"Pakistan\", \"Lahore\"):(\"Russia\", \"Moscow\")])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "69fc4098",
   "metadata": {},
   "source": [
    "                                date  avg_temp_c\n",
    "country  city                                   \n",
    "Pakistan Faisalabad       2000-01-01      12.792\n",
    "         Faisalabad       2000-02-01      14.339\n",
    "         Faisalabad       2000-03-01      20.309\n",
    "         Faisalabad       2000-04-01      29.072\n",
    "         Faisalabad       2000-05-01      34.845\n",
    "...                              ...         ...\n",
    "Russia   Saint Petersburg 2013-05-01      12.355\n",
    "         Saint Petersburg 2013-06-01      17.185\n",
    "         Saint Petersburg 2013-07-01      17.234\n",
    "         Saint Petersburg 2013-08-01      17.153\n",
    "         Saint Petersburg 2013-09-01         NaN\n",
    "\n",
    "[1155 rows x 2 columns]\n",
    "                         date  avg_temp_c\n",
    "country city                             \n",
    "Mexico  Mexico     2000-01-01      12.694\n",
    "        Mexico     2000-02-01      14.677\n",
    "        Mexico     2000-03-01      17.376\n",
    "        Mexico     2000-04-01      18.294\n",
    "        Mexico     2000-05-01      18.562\n",
    "...                       ...         ...\n",
    "Morocco Casablanca 2013-05-01      19.217\n",
    "        Casablanca 2013-06-01      23.649\n",
    "        Casablanca 2013-07-01      27.488\n",
    "        Casablanca 2013-08-01      27.952\n",
    "        Casablanca 2013-09-01         NaN\n",
    "\n",
    "[330 rows x 2 columns]\n",
    "                      date  avg_temp_c\n",
    "country  city                         \n",
    "Pakistan Lahore 2000-01-01      12.792\n",
    "         Lahore 2000-02-01      14.339\n",
    "         Lahore 2000-03-01      20.309\n",
    "         Lahore 2000-04-01      29.072\n",
    "         Lahore 2000-05-01      34.845\n",
    "...                    ...         ...\n",
    "Russia   Moscow 2013-05-01      16.152\n",
    "         Moscow 2013-06-01      18.718\n",
    "         Moscow 2013-07-01      18.136\n",
    "         Moscow 2013-08-01      17.485\n",
    "         Moscow 2013-09-01         NaN\n",
    "\n",
    "[660 rows x 2 columns]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "61ee111f",
   "metadata": {},
   "source": [
    "# Subset rows from India, Hyderabad to Iraq, Baghdad\n",
    "print(temperatures_srt.loc[('India', 'Hyderabad'):('Iraq', 'Baghdad')])\n",
    "\n",
    "# Subset columns from date to avg_temp_c\n",
    "print(temperatures_srt.loc[:, 'date':'avg_temp_c'])\n",
    "\n",
    "# Subset in both directions at once\n",
    "print(temperatures_srt.loc[('India', 'Hyderabad'):('Iraq', 'Baghdad'), 'date':'avg_temp_c'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a15f749a",
   "metadata": {},
   "source": [
    "                        date  avg_temp_c\n",
    "country city                            \n",
    "India   Hyderabad 2000-01-01      23.779\n",
    "        Hyderabad 2000-02-01      25.826\n",
    "        Hyderabad 2000-03-01      28.821\n",
    "        Hyderabad 2000-04-01      32.698\n",
    "        Hyderabad 2000-05-01      32.438\n",
    "...                      ...         ...\n",
    "Iraq    Baghdad   2013-05-01      28.673\n",
    "        Baghdad   2013-06-01      33.803\n",
    "        Baghdad   2013-07-01      36.392\n",
    "        Baghdad   2013-08-01      35.463\n",
    "        Baghdad   2013-09-01         NaN\n",
    "\n",
    "[2145 rows x 2 columns]\n",
    "                         date  avg_temp_c\n",
    "country     city                         \n",
    "Afghanistan Kabul  2000-01-01       3.326\n",
    "            Kabul  2000-02-01       3.454\n",
    "            Kabul  2000-03-01       9.612\n",
    "            Kabul  2000-04-01      17.925\n",
    "            Kabul  2000-05-01      24.658\n",
    "...                       ...         ...\n",
    "Zimbabwe    Harare 2013-05-01      18.298\n",
    "            Harare 2013-06-01      17.020\n",
    "            Harare 2013-07-01      16.299\n",
    "            Harare 2013-08-01      19.232\n",
    "            Harare 2013-09-01         NaN\n",
    "\n",
    "[16500 rows x 2 columns]\n",
    "                        date  avg_temp_c\n",
    "country city                            \n",
    "India   Hyderabad 2000-01-01      23.779\n",
    "        Hyderabad 2000-02-01      25.826\n",
    "        Hyderabad 2000-03-01      28.821\n",
    "        Hyderabad 2000-04-01      32.698\n",
    "        Hyderabad 2000-05-01      32.438\n",
    "...                      ...         ...\n",
    "Iraq    Baghdad   2013-05-01      28.673\n",
    "        Baghdad   2013-06-01      33.803\n",
    "        Baghdad   2013-07-01      36.392\n",
    "        Baghdad   2013-08-01      35.463\n",
    "        Baghdad   2013-09-01         NaN\n",
    "\n",
    "[2145 rows x 2 columns]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "12626510",
   "metadata": {},
   "source": [
    "# Use Boolean conditions to subset temperatures for rows in 2010 and 2011\n",
    "temperatures_bool = temperatures[(temperatures['date'] >= '2010-01-01') & (temperatures['date'] <= '2011-12-31')]\n",
    "print(temperatures_bool)\n",
    "\n",
    "# Set date as the index and sort the index\n",
    "temperatures_ind = temperatures.set_index('date').sort_index()\n",
    "\n",
    "# Use .loc[] to subset temperatures_ind for rows in 2010 and 2011\n",
    "print(temperatures_ind.loc['2010-01-01':'2011-12-31'])\n",
    "\n",
    "# Use .loc[] to subset temperatures_ind for rows from Aug 2010 to Feb 2011\n",
    "print(temperatures_ind.loc['2010-08-01':'2011-02-28'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8826f3c5",
   "metadata": {},
   "source": [
    "            date     city        country  avg_temp_c\n",
    "120   2010-01-01  Abidjan  Côte D'Ivoire      28.270\n",
    "121   2010-02-01  Abidjan  Côte D'Ivoire      29.262\n",
    "122   2010-03-01  Abidjan  Côte D'Ivoire      29.596\n",
    "123   2010-04-01  Abidjan  Côte D'Ivoire      29.068\n",
    "124   2010-05-01  Abidjan  Côte D'Ivoire      28.258\n",
    "...          ...      ...            ...         ...\n",
    "16474 2011-08-01     Xian          China      23.069\n",
    "16475 2011-09-01     Xian          China      16.775\n",
    "16476 2011-10-01     Xian          China      12.587\n",
    "16477 2011-11-01     Xian          China       7.543\n",
    "16478 2011-12-01     Xian          China      -0.490\n",
    "\n",
    "[2400 rows x 4 columns]\n",
    "                  city    country  avg_temp_c\n",
    "date                                         \n",
    "2010-01-01  Faisalabad   Pakistan      11.810\n",
    "2010-01-01   Melbourne  Australia      20.016\n",
    "2010-01-01   Chongqing      China       7.921\n",
    "2010-01-01   São Paulo     Brazil      23.738\n",
    "2010-01-01   Guangzhou      China      14.136\n",
    "...                ...        ...         ...\n",
    "2011-12-01      Nagoya      Japan       6.476\n",
    "2011-12-01   Hyderabad      India      23.613\n",
    "2011-12-01        Cali   Colombia      21.559\n",
    "2011-12-01        Lima       Peru      18.293\n",
    "2011-12-01     Bangkok   Thailand      25.021\n",
    "\n",
    "[2400 rows x 3 columns]\n",
    "                city        country  avg_temp_c\n",
    "date                                           \n",
    "2010-08-01  Calcutta          India      30.226\n",
    "2010-08-01      Pune          India      24.941\n",
    "2010-08-01     Izmir         Turkey      28.352\n",
    "2010-08-01   Tianjin          China      25.543\n",
    "2010-08-01    Manila    Philippines      27.101\n",
    "...              ...            ...         ...\n",
    "2011-02-01     Kabul    Afghanistan       3.914\n",
    "2011-02-01   Chicago  United States       0.276\n",
    "2011-02-01    Aleppo          Syria       8.246\n",
    "2011-02-01     Delhi          India      18.136\n",
    "2011-02-01   Rangoon          Burma      26.631\n",
    "\n",
    "[700 rows x 3 columns]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ceb9faf9",
   "metadata": {},
   "source": [
    "# Get 23rd row, 2nd column (index 22, 1)\n",
    "print(temperatures.iloc[22, 1])\n",
    "\n",
    "# Use slicing to get the first 5 rows\n",
    "print(temperatures.iloc[0:5])\n",
    "\n",
    "# Use slicing to get columns 3 to 4\n",
    "print(temperatures.iloc[:, 2:4])\n",
    "\n",
    "# Use slicing in both directions at once\n",
    "print(temperatures.iloc[0:5, 2:4])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c0e6f0da",
   "metadata": {},
   "source": [
    "Abidjan\n",
    "        date     city        country  avg_temp_c\n",
    "0 2000-01-01  Abidjan  Côte D'Ivoire      27.293\n",
    "1 2000-02-01  Abidjan  Côte D'Ivoire      27.685\n",
    "2 2000-03-01  Abidjan  Côte D'Ivoire      29.061\n",
    "3 2000-04-01  Abidjan  Côte D'Ivoire      28.162\n",
    "4 2000-05-01  Abidjan  Côte D'Ivoire      27.547\n",
    "             country  avg_temp_c\n",
    "0      Côte D'Ivoire      27.293\n",
    "1      Côte D'Ivoire      27.685\n",
    "2      Côte D'Ivoire      29.061\n",
    "3      Côte D'Ivoire      28.162\n",
    "4      Côte D'Ivoire      27.547\n",
    "...              ...         ...\n",
    "16495          China      18.979\n",
    "16496          China      23.522\n",
    "16497          China      25.251\n",
    "16498          China      24.528\n",
    "16499          China         NaN\n",
    "\n",
    "[16500 rows x 2 columns]\n",
    "         country  avg_temp_c\n",
    "0  Côte D'Ivoire      27.293\n",
    "1  Côte D'Ivoire      27.685\n",
    "2  Côte D'Ivoire      29.061\n",
    "3  Côte D'Ivoire      28.162\n",
    "4  Côte D'Ivoire      27.547"
   ]
  },
  {
   "cell_type": "raw",
   "id": "85d48bf8",
   "metadata": {},
   "source": [
    "# Add a year column to temperatures\n",
    "temperatures['year'] = pd.to_datetime(temperatures['date']).dt.year\n",
    "\n",
    "# Pivot avg_temp_c by country and city vs year\n",
    "temp_by_country_city_vs_year = temperatures.pivot_table(values='avg_temp_c', index=['country', 'city'], columns='year')\n",
    "\n",
    "# See the result\n",
    "print(temp_by_country_city_vs_year)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cb3fc70a",
   "metadata": {},
   "source": [
    "year                              2000    2001    2002    2003    2004  ...    2009    2010    2011    2012    2013\n",
    "country       city                                                      ...                                        \n",
    "Afghanistan   Kabul             15.823  15.848  15.715  15.133  16.128  ...  15.093  15.676  15.812  14.510  16.206\n",
    "Angola        Luanda            24.410  24.427  24.791  24.867  24.216  ...  24.325  24.440  24.151  24.240  24.554\n",
    "Australia     Melbourne         14.320  14.180  14.076  13.986  13.742  ...  14.647  14.232  14.191  14.269  14.742\n",
    "              Sydney            17.567  17.854  17.734  17.592  17.870  ...  18.176  17.999  17.713  17.474  18.090\n",
    "Bangladesh    Dhaka             25.905  25.931  26.095  25.927  26.136  ...  26.536  26.648  25.803  26.284  26.587\n",
    "...                                ...     ...     ...     ...     ...  ...     ...     ...     ...     ...     ...\n",
    "United States Chicago           11.090  11.703  11.532  10.482  10.943  ...  10.298  11.816  11.214  12.821  11.587\n",
    "              Los Angeles       16.643  16.466  16.430  16.945  16.553  ...  16.677  15.887  15.875  17.090  18.121\n",
    "              New York           9.969  10.931  11.252   9.836  10.389  ...  10.142  11.358  11.272  11.971  12.164\n",
    "Vietnam       Ho Chi Minh City  27.589  27.832  28.065  27.828  27.687  ...  27.853  28.282  27.675  28.249  28.455\n",
    "Zimbabwe      Harare            20.284  20.861  21.079  20.889  20.308  ...  20.524  21.166  20.782  20.523  19.756\n",
    "\n",
    "[100 rows x 14 columns]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "729b1358",
   "metadata": {},
   "source": [
    "# Subset for Egypt to India\n",
    "subset_egypt_to_india = temp_by_country_city_vs_year.loc['Egypt':'India']\n",
    "\n",
    "# Subset for Egypt, Cairo to India, Delhi\n",
    "subset_egypt_cairo_to_india_delhi = temp_by_country_city_vs_year.loc[('Egypt', 'Cairo'):('India', 'Delhi')]\n",
    "\n",
    "# Subset for Egypt, Cairo to India, Delhi, and 2005 to 2010\n",
    "subset_egypt_cairo_to_india_delhi_2005_2010 = temp_by_country_city_vs_year.loc[('Egypt', 'Cairo'):('India', 'Delhi'), 2005:2010]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "50679ca1",
   "metadata": {},
   "source": [
    "# Get the worldwide mean temp by year\n",
    "mean_temp_by_year = temp_by_country_city_vs_year.mean(axis=0)\n",
    "\n",
    "# Filter for the year that had the highest mean temp\n",
    "print(mean_temp_by_year[mean_temp_by_year.idxmax()])\n",
    "\n",
    "# Get the mean temp by city\n",
    "mean_temp_by_city = temp_by_country_city_vs_year.mean(axis=1)\n",
    "\n",
    "# Filter for the city that had the lowest mean temp\n",
    "print(mean_temp_by_city[mean_temp_by_city.idxmin()])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2a8cfb17",
   "metadata": {},
   "source": [
    "20.312285416666665\n",
    "4.876550595238094"
   ]
  },
  {
   "cell_type": "raw",
   "id": "05d154d4",
   "metadata": {},
   "source": [
    "# Get the worldwide mean temp by year\n",
    "mean_temp_by_year = temp_by_country_city_vs_year.mean(axis=0)\n",
    "\n",
    "# Filter for the year that had the highest mean temp\n",
    "print(mean_temp_by_year[mean_temp_by_year == mean_temp_by_year.max()])\n",
    "\n",
    "# Get the mean temp by city\n",
    "mean_temp_by_city = temp_by_country_city_vs_year.mean(axis=1)\n",
    "\n",
    "# Filter for the city that had the lowest mean temp\n",
    "print(mean_temp_by_city[mean_temp_by_city == mean_temp_by_city.min()])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa1df9a8",
   "metadata": {},
   "source": [
    "year\n",
    "2013    20.312\n",
    "dtype: float64\n",
    "country  city  \n",
    "China    Harbin    4.877\n",
    "dtype: float64"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8bf35b95",
   "metadata": {},
   "source": [
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Look at the first few rows of data\n",
    "print(avocados.head())\n",
    "\n",
    "# Get the total number of avocados sold of each size\n",
    "nb_sold_by_size = avocados.groupby('size')['nb_sold'].sum()\n",
    "\n",
    "# Create a bar plot of the number of avocados sold by size\n",
    "nb_sold_by_size.plot(kind='bar', figsize=(10, 6), color='skyblue')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "867746ab",
   "metadata": {},
   "source": [
    "         date          type  year  avg_price   size    nb_sold\n",
    "0  2015-12-27  conventional  2015       0.95  small  9.627e+06\n",
    "1  2015-12-20  conventional  2015       0.98  small  8.710e+06\n",
    "2  2015-12-13  conventional  2015       0.93  small  9.855e+06\n",
    "3  2015-12-06  conventional  2015       0.89  small  9.405e+06\n",
    "4  2015-11-29  conventional  2015       0.99  small  8.095e+06"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0c79306e",
   "metadata": {},
   "source": [
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the total number of avocados sold on each date\n",
    "nb_sold_by_date = avocados.groupby(\"date\")[\"nb_sold\"].sum()\n",
    "\n",
    "# Create a line plot of the number of avocados sold by date\n",
    "nb_sold_by_date.plot(kind='line', figsize=(12, 6), marker='o', linestyle='-', color='b')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "006282bf",
   "metadata": {},
   "source": [
    "# Scatter plot of avg_price vs. nb_sold with title\n",
    "avocados.plot(x=\"nb_sold\", y=\"avg_price\", kind=\"scatter\", title=\"Number of avocados sold vs. average price\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cb3cf1e4",
   "metadata": {},
   "source": [
    "# Histogram of conventional avg_price \n",
    "avocados[avocados[\"type\"] == \"conventional\"][\"avg_price\"].hist()\n",
    "\n",
    "# Histogram of organic avg_price\n",
    "avocados[avocados[\"type\"] == \"organic\"][\"avg_price\"].hist()\n",
    "\n",
    "# Add a legend\n",
    "plt.legend(['conventional', 'organic'])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4938cee9",
   "metadata": {},
   "source": [
    "# Modify histogram transparency to 0.5 \n",
    "avocados[avocados[\"type\"] == \"conventional\"][\"avg_price\"].hist(alpha=0.5)\n",
    "\n",
    "# Modify histogram transparency to 0.5\n",
    "avocados[avocados[\"type\"] == \"organic\"][\"avg_price\"].hist(alpha=0.5)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend([\"conventional\", \"organic\"])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "27a1ff3e",
   "metadata": {},
   "source": [
    "# Modify bins to 20\n",
    "avocados[avocados[\"type\"] == \"conventional\"][\"avg_price\"].hist(alpha=0.5, bins=20)\n",
    "\n",
    "# Modify bins to 20\n",
    "avocados[avocados[\"type\"] == \"organic\"][\"avg_price\"].hist(alpha=0.5, bins=20)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend([\"conventional\", \"organic\"])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "08c1b947",
   "metadata": {},
   "source": [
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check individual values for missing values\n",
    "print(avocados_2016.isna())\n",
    "\n",
    "# Check each column for missing values\n",
    "print(avocados_2016.isna().any())\n",
    "\n",
    "# Bar plot of missing values by variable\n",
    "avocados_2016.isna().sum().plot(kind=\"bar\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9e14bfb5",
   "metadata": {},
   "source": [
    "     date  avg_price  total_sold  small_sold  large_sold  xl_sold  total_bags_sold  small_bags_sold  large_bags_sold  xl_bags_sold\n",
    "0   False      False       False       False       False    False            False            False            False         False\n",
    "1   False      False       False       False       False    False            False            False            False         False\n",
    "2   False      False       False       False        True    False            False            False            False         False\n",
    "3   False      False       False       False       False    False            False            False            False         False\n",
    "4   False      False       False       False       False     True            False            False            False         False\n",
    "5   False      False       False        True       False    False            False            False            False         False\n",
    "6   False      False       False       False       False    False            False            False            False         False\n",
    "7   False      False       False       False        True    False            False            False            False         False\n",
    "8   False      False       False       False       False    False            False            False            False         False\n",
    "9   False      False       False       False       False    False            False            False            False         False\n",
    "10  False      False       False       False        True    False            False            False            False         False\n",
    "11  False      False       False       False       False    False            False            False            False         False\n",
    "12  False      False       False       False       False    False            False            False            False         False\n",
    "13  False      False       False       False       False    False            False            False            False         False\n",
    "14  False      False       False       False       False    False            False            False            False         False\n",
    "15  False      False       False       False        True    False            False            False            False         False\n",
    "16  False      False       False       False       False     True            False            False            False         False\n",
    "17  False      False       False       False       False    False            False            False            False         False\n",
    "18  False      False       False       False       False    False            False            False            False         False\n",
    "19  False      False       False       False        True    False            False            False            False         False\n",
    "20  False      False       False       False       False    False            False            False            False         False\n",
    "21  False      False       False       False       False    False            False            False            False         False\n",
    "22  False      False       False       False       False    False            False            False            False         False\n",
    "23  False      False       False       False       False    False            False            False            False         False\n",
    "24  False      False       False       False       False    False            False            False            False         False\n",
    "25  False      False       False       False       False    False            False            False            False         False\n",
    "26  False      False       False       False       False    False            False            False            False         False\n",
    "27  False      False       False       False       False    False            False            False            False         False\n",
    "28  False      False       False       False       False    False            False            False            False         False\n",
    "29  False      False       False       False       False    False            False            False            False         False\n",
    "30  False      False       False       False       False     True            False            False            False         False\n",
    "31  False      False       False       False       False    False            False            False            False         False\n",
    "32  False      False       False       False       False     True            False            False            False         False\n",
    "33  False      False       False       False       False    False            False            False            False         False\n",
    "34  False      False       False       False       False    False            False            False            False         False\n",
    "35  False      False       False       False       False    False            False            False            False         False\n",
    "36  False      False       False        True       False    False            False            False            False         False\n",
    "37  False      False       False       False        True    False            False            False            False         False\n",
    "38  False      False       False       False       False    False            False            False            False         False\n",
    "39  False      False       False       False       False    False            False            False            False         False\n",
    "40  False      False       False        True       False    False            False            False            False         False\n",
    "41  False      False       False       False       False    False            False            False            False         False\n",
    "42  False      False       False       False       False    False            False            False            False         False\n",
    "43  False      False       False       False       False    False            False            False            False         False\n",
    "44  False      False       False        True       False    False            False            False            False         False\n",
    "45  False      False       False       False       False    False            False            False            False         False\n",
    "46  False      False       False       False       False    False            False            False            False         False\n",
    "47  False      False       False       False       False    False            False            False            False         False\n",
    "48  False      False       False       False       False    False            False            False            False         False\n",
    "49  False      False       False       False       False    False            False            False            False         False\n",
    "50  False      False       False        True       False    False            False            False            False         False\n",
    "51  False      False       False        True       False    False            False            False            False         False\n",
    "date               False\n",
    "avg_price          False\n",
    "total_sold         False\n",
    "small_sold          True\n",
    "large_sold          True\n",
    "xl_sold             True\n",
    "total_bags_sold    False\n",
    "small_bags_sold    False\n",
    "large_bags_sold    False\n",
    "xl_bags_sold       False\n",
    "dtype: bool"
   ]
  },
  {
   "cell_type": "raw",
   "id": "224d6b36",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "avocados_2016 = pd.read_pickle(\"/usr/local/share/datasets/avos_2016.pkl\")\n",
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check individual values for missing values\n",
    "print(avocados_2016.isna())\n",
    "\n",
    "# Check each column for missing values\n",
    "print(avocados_2016.isna().any())\n",
    "\n",
    "# Bar plot of missing values by variable\n",
    "missing_values_count = avocados_2016.isna().sum()\n",
    "missing_values_count.plot(kind=\"bar\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dde1ff28",
   "metadata": {},
   "source": [
    "# Remove rows with missing values\n",
    "avocados_complete = avocados_2016.dropna()\n",
    "\n",
    "# Check if any columns contain missing values\n",
    "print(avocados_complete.isna().any())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e8016f8c",
   "metadata": {},
   "source": [
    "date               0\n",
    "avg_price          0\n",
    "total_sold         0\n",
    "small_sold         0\n",
    "large_sold         0\n",
    "xl_sold            0\n",
    "total_bags_sold    0\n",
    "small_bags_sold    0\n",
    "large_bags_sold    0\n",
    "xl_bags_sold       0\n",
    "dtype: int64"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c9d244a2",
   "metadata": {},
   "source": [
    "# List the columns with missing values\n",
    "cols_with_missing = [\"small_sold\", \"large_sold\", \"xl_sold\"]\n",
    "\n",
    "# Create histograms showing the distributions cols_with_missing\n",
    "avocados_2016[cols_with_missing].hist()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4cd4b377",
   "metadata": {},
   "source": [
    "# From previous step\n",
    "cols_with_missing = [\"small_sold\", \"large_sold\", \"xl_sold\"]\n",
    "avocados_2016[cols_with_missing].hist()\n",
    "plt.show()\n",
    "\n",
    "# Fill in missing values with 0\n",
    "avocados_filled = avocados_2016.fillna(0)\n",
    "\n",
    "# Create histograms of the filled columns\n",
    "avocados_filled[cols_with_missing].hist()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1e4a20ac",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "# Create a list of dictionaries with new data\n",
    "avocados_list = [\n",
    "    {\"date\": \"2019-11-03\", \"small_sold\": 10376832, \"large_sold\": 7835071},\n",
    "    {\"date\": \"2019-11-10\", \"small_sold\": 10717154, \"large_sold\": 8561348},\n",
    "]\n",
    "\n",
    "# Convert list into DataFrame\n",
    "avocados_2019 = pd.DataFrame(avocados_list)\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(avocados_2019)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "09e11ec5",
   "metadata": {},
   "source": [
    "         date  small_sold  large_sold\n",
    "0  2019-11-03    10376832     7835071\n",
    "1  2019-11-10    10717154     8561348"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5691e009",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "# Create a dictionary of lists with new data\n",
    "avocados_dict = {\n",
    "  \"date\": [\"2019-11-17\", \"2019-12-01\"],\n",
    "  \"small_sold\": [10859987, 9291631],\n",
    "  \"large_sold\": [7674135, 6238096]\n",
    "}\n",
    "\n",
    "# Convert dictionary into DataFrame\n",
    "avocados_2019 = pd.DataFrame(avocados_dict)\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(avocados_2019)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c715f00",
   "metadata": {},
   "source": [
    "         date  small_sold  large_sold\n",
    "0  2019-11-17    10859987     7674135\n",
    "1  2019-12-01     9291631     6238096"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ffce1ea9",
   "metadata": {},
   "source": [
    "# Read CSV as DataFrame called airline_bumping\n",
    "airline_bumping = pd.read_csv(\"airline_bumping.csv\")\n",
    "\n",
    "# Take a look at the DataFrame\n",
    "print(airline_bumping.head())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "92906fab",
   "metadata": {},
   "source": [
    "             airline  year  nb_bumped  total_passengers\n",
    "0    DELTA AIR LINES  2017        679          99796155\n",
    "1     VIRGIN AMERICA  2017        165           6090029\n",
    "2    JETBLUE AIRWAYS  2017       1475          27255038\n",
    "3    UNITED AIRLINES  2017       2067          70030765\n",
    "4  HAWAIIAN AIRLINES  2017         92           8422734"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c9d55f8",
   "metadata": {},
   "source": [
    "# From previous step\n",
    "airline_bumping = pd.read_csv(\"airline_bumping.csv\")\n",
    "print(airline_bumping.head())\n",
    "\n",
    "# For each airline, select nb_bumped and total_passengers and sum\n",
    "airline_totals = airline_bumping.groupby('airline')[['nb_bumped', 'total_passengers']].sum()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0094a214",
   "metadata": {},
   "source": [
    "             airline  year  nb_bumped  total_passengers\n",
    "0    DELTA AIR LINES  2017        679          99796155\n",
    "1     VIRGIN AMERICA  2017        165           6090029\n",
    "2    JETBLUE AIRWAYS  2017       1475          27255038\n",
    "3    UNITED AIRLINES  2017       2067          70030765\n",
    "4  HAWAIIAN AIRLINES  2017         92           8422734"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e9fedcf",
   "metadata": {},
   "source": [
    "# From previous steps\n",
    "airline_bumping = pd.read_csv(\"airline_bumping.csv\")\n",
    "print(airline_bumping.head())\n",
    "airline_totals = airline_bumping.groupby(\"airline\")[[\"nb_bumped\", \"total_passengers\"]].sum()\n",
    "\n",
    "# Create new col, bumps_per_10k: no. of bumps per 10k passengers for each airline\n",
    "airline_totals[\"bumps_per_10k\"] = airline_totals[\"nb_bumped\"] / airline_totals[\"total_passengers\"] * 10000"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ee820238",
   "metadata": {},
   "source": [
    "             airline  year  nb_bumped  total_passengers\n",
    "0    DELTA AIR LINES  2017        679          99796155\n",
    "1     VIRGIN AMERICA  2017        165           6090029\n",
    "2    JETBLUE AIRWAYS  2017       1475          27255038\n",
    "3    UNITED AIRLINES  2017       2067          70030765\n",
    "4  HAWAIIAN AIRLINES  2017         92           8422734"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1fddfd15",
   "metadata": {},
   "source": [
    "# Print airline_totals\n",
    "print(airline_totals)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "22c17774",
   "metadata": {},
   "source": [
    "             airline  year  nb_bumped  total_passengers\n",
    "0    DELTA AIR LINES  2017        679          99796155\n",
    "1     VIRGIN AMERICA  2017        165           6090029\n",
    "2    JETBLUE AIRWAYS  2017       1475          27255038\n",
    "3    UNITED AIRLINES  2017       2067          70030765\n",
    "4  HAWAIIAN AIRLINES  2017         92           8422734\n",
    "                     nb_bumped  total_passengers  bumps_per_10k\n",
    "airline                                                        \n",
    "ALASKA AIRLINES           1392          36543121          0.381\n",
    "AMERICAN AIRLINES        11115         197365225          0.563\n",
    "DELTA AIR LINES           1591         197033215          0.081\n",
    "EXPRESSJET AIRLINES       3326          27858678          1.194\n",
    "FRONTIER AIRLINES         1228          22954995          0.535\n",
    "HAWAIIAN AIRLINES          122          16577572          0.074\n",
    "JETBLUE AIRWAYS           3615          53245866          0.679\n",
    "SKYWEST AIRLINES          3094          47091737          0.657\n",
    "SOUTHWEST AIRLINES       18585         228142036          0.815\n",
    "SPIRIT AIRLINES           2920          32304571          0.904\n",
    "UNITED AIRLINES           4941         134468897          0.367\n",
    "VIRGIN AMERICA             242          12017967          0.201"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1555fd4f",
   "metadata": {},
   "source": [
    "# Create airline_totals_sorted\n",
    "airline_totals_sorted = airline_totals.sort_values(by=\"bumps_per_10k\", ascending=False)\n",
    "\n",
    "# Print airline_totals_sorted\n",
    "print(airline_totals_sorted)\n",
    "\n",
    "# Save as airline_totals_sorted.csv\n",
    "airline_totals_sorted.to_csv(\"airline_totals_sorted.csv\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "652acc5a",
   "metadata": {},
   "source": [
    "                     nb_bumped  total_passengers  bumps_per_10k\n",
    "airline                                                        \n",
    "EXPRESSJET AIRLINES       3326          27858678          1.194\n",
    "SPIRIT AIRLINES           2920          32304571          0.904\n",
    "SOUTHWEST AIRLINES       18585         228142036          0.815\n",
    "JETBLUE AIRWAYS           3615          53245866          0.679\n",
    "SKYWEST AIRLINES          3094          47091737          0.657\n",
    "AMERICAN AIRLINES        11115         197365225          0.563\n",
    "FRONTIER AIRLINES         1228          22954995          0.535\n",
    "ALASKA AIRLINES           1392          36543121          0.381\n",
    "UNITED AIRLINES           4941         134468897          0.367\n",
    "VIRGIN AMERICA             242          12017967          0.201\n",
    "DELTA AIR LINES           1591         197033215          0.081\n",
    "HAWAIIAN AIRLINES          122          16577572          0.074"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499ec0e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

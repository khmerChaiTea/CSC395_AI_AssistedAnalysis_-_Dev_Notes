{
 "cells": [
  {
   "cell_type": "raw",
   "id": "e6acaf88",
   "metadata": {},
   "source": [
    "# Sample 1000 rows from spotify_population\n",
    "spotify_sample = spotify_population.sample(n=1000)\n",
    "\n",
    "# Print the sample\n",
    "print(spotify_sample)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b63da3eb",
   "metadata": {},
   "source": [
    "       acousticness                           artists  danceability  duration_ms  duration_minutes  ...  release_date  speechiness    tempo  valence    year\n",
    "4132      3.590e-02                 ['Natanael Cano']         0.607     115793.0             1.930  ...    2020-06-12        0.047  127.619    0.434  2020.0\n",
    "15898     3.800e-03               ['Imagine Dragons']         0.685     206707.0             3.445  ...    2012-09-04        0.042   99.983    0.570  2012.0\n",
    "31344     6.910e-04                        ['*NSYNC']         0.665     228907.0             3.815  ...    2000-03-21        0.058  111.045    0.629  2000.0\n",
    "9211      6.850e-01                       ['Hammock']         0.114     308907.0             5.148  ...    2008-08-06        0.046   77.945    0.040  2008.0\n",
    "9611      2.260e-03           ['Motionless In White']         0.245     236973.0             3.950  ...    2012-11-13        0.191  135.023    0.171  2012.0\n",
    "...             ...                               ...           ...          ...               ...  ...           ...          ...      ...      ...     ...\n",
    "6206      5.410e-01  ['Turbo', 'Gunna', 'Young Thug']         0.787     217728.0             3.629  ...    2020-04-02        0.268   91.344    0.327  2020.0\n",
    "38218     4.510e-02                 ['Mayday Parade']         0.261     276173.0             4.603  ...    2007-07-10        0.032  139.242    0.329  2007.0\n",
    "27788     2.490e-01                 ['The Dramatics']         0.641     214973.0             3.583  ...    2007-01-01        0.118  152.939    0.914  2007.0\n",
    "30031     1.700e-02         ['Lil Wayne', 'Babyface']         0.596     265240.0             4.421  ...    2008-06-10        0.075   94.197    0.693  2008.0\n",
    "18515     8.720e-01         ['$NOT', 'SUS Valentino']         0.566     114547.0             1.909  ...    2018-03-25        0.290   87.020    0.552  2018.0\n",
    "\n",
    "[1000 rows x 20 columns]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c9821c3c",
   "metadata": {},
   "source": [
    "# Sample 1000 rows from spotify_population\n",
    "spotify_sample = spotify_population.sample(n=1000)\n",
    "\n",
    "# Print the sample\n",
    "print(spotify_sample)\n",
    "\n",
    "# Calculate the mean duration in mins from spotify_population\n",
    "mean_dur_pop = spotify_population['duration_minutes'].mean()\n",
    "\n",
    "# Calculate the mean duration in mins from spotify_sample\n",
    "mean_dur_samp = spotify_sample['duration_minutes'].mean()\n",
    "\n",
    "# Print the means\n",
    "print(mean_dur_pop)\n",
    "print(mean_dur_samp)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f1d80832",
   "metadata": {},
   "source": [
    "       acousticness                           artists  danceability  duration_ms  duration_minutes  ...  release_date  speechiness    tempo  valence    year\n",
    "4132      3.590e-02                 ['Natanael Cano']         0.607     115793.0             1.930  ...    2020-06-12        0.047  127.619    0.434  2020.0\n",
    "15898     3.800e-03               ['Imagine Dragons']         0.685     206707.0             3.445  ...    2012-09-04        0.042   99.983    0.570  2012.0\n",
    "31344     6.910e-04                        ['*NSYNC']         0.665     228907.0             3.815  ...    2000-03-21        0.058  111.045    0.629  2000.0\n",
    "9211      6.850e-01                       ['Hammock']         0.114     308907.0             5.148  ...    2008-08-06        0.046   77.945    0.040  2008.0\n",
    "9611      2.260e-03           ['Motionless In White']         0.245     236973.0             3.950  ...    2012-11-13        0.191  135.023    0.171  2012.0\n",
    "...             ...                               ...           ...          ...               ...  ...           ...          ...      ...      ...     ...\n",
    "6206      5.410e-01  ['Turbo', 'Gunna', 'Young Thug']         0.787     217728.0             3.629  ...    2020-04-02        0.268   91.344    0.327  2020.0\n",
    "38218     4.510e-02                 ['Mayday Parade']         0.261     276173.0             4.603  ...    2007-07-10        0.032  139.242    0.329  2007.0\n",
    "27788     2.490e-01                 ['The Dramatics']         0.641     214973.0             3.583  ...    2007-01-01        0.118  152.939    0.914  2007.0\n",
    "30031     1.700e-02         ['Lil Wayne', 'Babyface']         0.596     265240.0             4.421  ...    2008-06-10        0.075   94.197    0.693  2008.0\n",
    "18515     8.720e-01         ['$NOT', 'SUS Valentino']         0.566     114547.0             1.909  ...    2018-03-25        0.290   87.020    0.552  2018.0\n",
    "\n",
    "[1000 rows x 20 columns]\n",
    "3.8521519140900073\n",
    "3.8277472833333333"
   ]
  },
  {
   "cell_type": "raw",
   "id": "abeeacbf",
   "metadata": {},
   "source": [
    "# Create a pandas Series from the loudness column of spotify_population\n",
    "loudness_pop = spotify_population['loudness']\n",
    "\n",
    "# Sample 100 values of loudness_pop\n",
    "loudness_samp = loudness_pop.sample(n=100)\n",
    "\n",
    "print(loudness_samp)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "14eb60fe",
   "metadata": {},
   "source": [
    "4132     -5.109\n",
    "15898    -4.568\n",
    "31344    -4.455\n",
    "9211    -15.954\n",
    "9611     -2.720\n",
    "          ...  \n",
    "568      -4.399\n",
    "10576    -7.245\n",
    "12134    -8.363\n",
    "35880    -4.488\n",
    "2646    -20.966\n",
    "Name: loudness, Length: 100, dtype: float64"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8a03ecf3",
   "metadata": {},
   "source": [
    "# Create a pandas Series from the loudness column of spotify_population\n",
    "loudness_pop = spotify_population['loudness']\n",
    "\n",
    "# Sample 100 values of loudness_pop\n",
    "loudness_samp = loudness_pop.sample(n=100)\n",
    "\n",
    "# Calculate the mean of loudness_pop\n",
    "mean_loudness_pop = np.mean(loudness_pop)\n",
    "\n",
    "# Calculate the mean of loudness_samp\n",
    "mean_loudness_samp = np.mean(loudness_samp)\n",
    "\n",
    "print(mean_loudness_pop)\n",
    "print(mean_loudness_samp)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0775b261",
   "metadata": {},
   "source": [
    "-7.366856851353947\n",
    "-7.50746"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c821c4d4",
   "metadata": {},
   "source": [
    "# Visualize the distribution of acousticness with a histogram\n",
    "spotify_population['acousticness'].hist(bins=np.arange(0, 1.01, 0.01))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3b9c56bb",
   "metadata": {},
   "source": [
    "# Update the histogram to use spotify_mysterious_sample\n",
    "spotify_mysterious_sample['acousticness'].hist(bins=np.arange(0, 1.01, 0.01))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b57aeece",
   "metadata": {},
   "source": [
    "# Visualize the distribution of duration_minutes as a histogram\n",
    "spotify_population['duration_minutes'].hist(bins=np.arange(0, 15.5, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "695aa869",
   "metadata": {},
   "source": [
    "# Update the histogram to use spotify_mysterious_sample2\n",
    "spotify_mysterious_sample2['duration_minutes'].hist(bins=np.arange(0, 15.5, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "16e34c56",
   "metadata": {},
   "source": [
    "# Generate random numbers from a Uniform(-3, 3)\n",
    "uniforms = np.random.uniform(low =-3, high =3, size=5000)\n",
    "\n",
    "# Print uniforms\n",
    "print(uniforms)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "533d30a2",
   "metadata": {},
   "source": [
    "[ 1.17881511 -1.28316399 -1.63889128 ... -0.41025441  2.65625875\n",
    " -2.73604158]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "53734ebe",
   "metadata": {},
   "source": [
    "# Generate random numbers from a Uniform(-3, 3)\n",
    "uniforms = np.random.uniform(low=-3, high=3, size=5000)\n",
    "\n",
    "# Generate random numbers from a Normal(5, 2)\n",
    "normals = np.random.normal(loc=5, scale=2, size=5000)\n",
    "\n",
    "# Print normals\n",
    "print(normals)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "df8d616b",
   "metadata": {},
   "source": [
    "[ 1.96858828  5.45561711  5.561821   ... 10.38934379  5.73819916\n",
    "  5.48765765]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "def23479",
   "metadata": {},
   "source": [
    "# Generate random numbers from a Uniform(-3, 3)\n",
    "uniforms = np.random.uniform(low=-3, high=3, size=5000)\n",
    "\n",
    "# Plot a histogram of uniform values, binwidth 0.25\n",
    "plt.hist(uniforms, bins=np.arange(-3, 3.25, 0.25))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0a8e1412",
   "metadata": {},
   "source": [
    "# Generate random numbers from a Normal(5, 2)\n",
    "normals = np.random.normal(loc=5, scale=2, size=5000)\n",
    "\n",
    "# Plot a histogram of normal values, binwidth 0.5\n",
    "plt.hist(normals, bins=np.arange(-2, 13.5, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "19553d0b",
   "metadata": {},
   "source": [
    "# Sample 70 rows using simple random sampling and set the seed\n",
    "attrition_samp = attrition_pop.sample(n=70, random_state=18900217)\n",
    "\n",
    "# Print the sample\n",
    "print(attrition_samp)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5d5b890c",
   "metadata": {},
   "source": [
    "      Age  Attrition     BusinessTravel  DailyRate            Department  ...  WorkLifeBalance YearsAtCompany YearsInCurrentRole YearsSinceLastPromotion YearsWithCurrManager\n",
    "1134   35        0.0      Travel_Rarely        583  Research_Development  ...             Good             16                 10                      10                    1\n",
    "1150   52        0.0         Non-Travel        585                 Sales  ...             Good              9                  8                       0                    0\n",
    "531    33        0.0      Travel_Rarely        931  Research_Development  ...           Better              8                  7                       1                    6\n",
    "395    31        0.0      Travel_Rarely       1332  Research_Development  ...             Good              6                  5                       0                    1\n",
    "392    29        0.0      Travel_Rarely        942  Research_Development  ...             Good              5                  4                       1                    3\n",
    "...   ...        ...                ...        ...                   ...  ...              ...            ...                ...                     ...                  ...\n",
    "361    27        0.0  Travel_Frequently       1410                 Sales  ...           Better              6                  5                       0                    4\n",
    "1180   36        0.0      Travel_Rarely        530                 Sales  ...             Good             13                  7                       6                    7\n",
    "230    26        0.0      Travel_Rarely       1443                 Sales  ...             Good              2                  2                       0                    0\n",
    "211    29        0.0  Travel_Frequently        410  Research_Development  ...           Better              3                  2                       0                    2\n",
    "890    30        0.0  Travel_Frequently       1312  Research_Development  ...           Better              9                  7                       0                    7\n",
    "\n",
    "[70 rows x 31 columns]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2b9b94b5",
   "metadata": {},
   "source": [
    "# Set the sample size to 70\n",
    "sample_size = 70\n",
    "\n",
    "# Calculate the population size from attrition_pop\n",
    "pop_size = len(attrition_pop)\n",
    "\n",
    "# Calculate the interval\n",
    "interval = pop_size // sample_size"
   ]
  },
  {
   "cell_type": "raw",
   "id": "19ef787a",
   "metadata": {},
   "source": [
    "# Set the sample size to 70\n",
    "sample_size = 70\n",
    "\n",
    "# Calculate the population size from attrition_pop\n",
    "pop_size = len(attrition_pop)\n",
    "\n",
    "# Calculate the interval\n",
    "interval = pop_size // sample_size\n",
    "\n",
    "# Systematically sample 70 rows\n",
    "attrition_sys_samp = attrition_pop.iloc[::interval]\n",
    "\n",
    "# Print the sample\n",
    "print(attrition_sys_samp)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b90f8157",
   "metadata": {},
   "source": [
    "      Age  Attrition BusinessTravel  DailyRate            Department  ...  WorkLifeBalance YearsAtCompany YearsInCurrentRole YearsSinceLastPromotion YearsWithCurrManager\n",
    "0      21        0.0  Travel_Rarely        391  Research_Development  ...           Better              0                  0                       0                    0\n",
    "21     19        0.0  Travel_Rarely       1181  Research_Development  ...           Better              1                  0                       0                    0\n",
    "42     45        0.0  Travel_Rarely        252  Research_Development  ...           Better              1                  0                       0                    0\n",
    "63     23        0.0  Travel_Rarely        373  Research_Development  ...           Better              1                  0                       0                    1\n",
    "84     30        1.0  Travel_Rarely        945                 Sales  ...             Good              1                  0                       0                    0\n",
    "...   ...        ...            ...        ...                   ...  ...              ...            ...                ...                     ...                  ...\n",
    "1365   48        0.0  Travel_Rarely        715  Research_Development  ...             Best              1                  0                       0                    0\n",
    "1386   48        0.0  Travel_Rarely       1355  Research_Development  ...           Better             15                 11                       4                    8\n",
    "1407   50        0.0  Travel_Rarely        989  Research_Development  ...             Good             27                  3                      13                    8\n",
    "1428   50        0.0     Non-Travel        881  Research_Development  ...           Better             31                  6                      14                    7\n",
    "1449   52        0.0  Travel_Rarely        699  Research_Development  ...           Better             33                 18                      11                    9\n",
    "\n",
    "[70 rows x 31 columns]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dd514b87",
   "metadata": {},
   "source": [
    "# Add an index column to attrition_pop\n",
    "attrition_pop_id = attrition_pop.reset_index()\n",
    "\n",
    "# Plot YearsAtCompany vs. index for attrition_pop_id\n",
    "attrition_pop_id.plot(x='index', y='YearsAtCompany', kind='scatter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b0112029",
   "metadata": {},
   "source": [
    "# Shuffle the rows of attrition_pop\n",
    "attrition_shuffled = attrition_pop.sample(frac=1)\n",
    "\n",
    "# Reset the row indexes and create an index column\n",
    "attrition_shuffled = attrition_shuffled.reset_index(drop=True).reset_index()\n",
    "\n",
    "# Plot YearsAtCompany vs. index for attrition_shuffled\n",
    "attrition_shuffled.plot(x='index', y='YearsAtCompany', kind='scatter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "96623f81",
   "metadata": {},
   "source": [
    "# Proportion of employees by Education level\n",
    "education_counts_pop = attrition_pop['Education'].value_counts(normalize=True)\n",
    "\n",
    "# Print education_counts_pop\n",
    "print(education_counts_pop)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1dc1cc25",
   "metadata": {},
   "source": [
    "Bachelor         0.389\n",
    "Master           0.271\n",
    "College          0.192\n",
    "Below_College    0.116\n",
    "Doctor           0.033\n",
    "Name: Education, dtype: float64"
   ]
  },
  {
   "cell_type": "raw",
   "id": "47d8e8be",
   "metadata": {},
   "source": [
    "# Proportion of employees by Education level\n",
    "education_counts_pop = attrition_pop['Education'].value_counts(normalize=True)\n",
    "\n",
    "# Print education_counts_pop\n",
    "print(education_counts_pop)\n",
    "\n",
    "# Proportional stratified sampling for 40% of each Education group\n",
    "attrition_strat = attrition_pop.groupby('Education')\\\n",
    "\t.sample(frac=0.4, random_state=2022)\n",
    "\n",
    "\n",
    "# Print the sample\n",
    "print(attrition_strat)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "af82cd96",
   "metadata": {},
   "source": [
    "Bachelor         0.389\n",
    "Master           0.271\n",
    "College          0.192\n",
    "Below_College    0.116\n",
    "Doctor           0.033\n",
    "Name: Education, dtype: float64\n",
    "      Age  Attrition     BusinessTravel  DailyRate            Department  ...  WorkLifeBalance YearsAtCompany YearsInCurrentRole YearsSinceLastPromotion YearsWithCurrManager\n",
    "1191   53        0.0      Travel_Rarely        238                 Sales  ...             Best             14                  7                       8                   10\n",
    "407    29        0.0  Travel_Frequently        995  Research_Development  ...             Best              6                  4                       1                    3\n",
    "1233   59        0.0  Travel_Frequently       1225                 Sales  ...             Good              4                  3                       1                    3\n",
    "366    37        0.0      Travel_Rarely        571  Research_Development  ...             Good              5                  3                       4                    3\n",
    "702    31        0.0  Travel_Frequently        163  Research_Development  ...             Good              5                  4                       1                    4\n",
    "...   ...        ...                ...        ...                   ...  ...              ...            ...                ...                     ...                  ...\n",
    "733    38        0.0  Travel_Frequently        653  Research_Development  ...           Better             10                  3                       9                    9\n",
    "1061   44        0.0  Travel_Frequently        602       Human_Resources  ...           Better             10                  7                       0                    2\n",
    "1307   41        0.0      Travel_Rarely       1276                 Sales  ...           Better             18                 16                      11                    8\n",
    "1060   33        0.0      Travel_Rarely        516  Research_Development  ...           Better              0                  0                       0                    0\n",
    "177    29        0.0      Travel_Rarely        738  Research_Development  ...           Better              3                  2                       2                    2\n",
    "\n",
    "[588 rows x 31 columns]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "82484e82",
   "metadata": {},
   "source": [
    "# Proportion of employees by Education level\n",
    "education_counts_pop = attrition_pop['Education'].value_counts(normalize=True)\n",
    "\n",
    "# Print education_counts_pop\n",
    "print(education_counts_pop)\n",
    "\n",
    "# Proportional stratified sampling for 40% of each Education group\n",
    "attrition_strat = attrition_pop.groupby('Education')\\\n",
    "\t.sample(frac=0.4, random_state=2022)\n",
    "\n",
    "# Calculate the Education level proportions from attrition_strat\n",
    "education_counts_strat = attrition_strat['Education'].value_counts(normalize=True)\n",
    "\n",
    "# Print education_counts_strat\n",
    "print(education_counts_strat)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5bbdb1e7",
   "metadata": {},
   "source": [
    "Bachelor         0.389\n",
    "Master           0.271\n",
    "College          0.192\n",
    "Below_College    0.116\n",
    "Doctor           0.033\n",
    "Name: Education, dtype: float64\n",
    "Bachelor         0.389\n",
    "Master           0.270\n",
    "College          0.192\n",
    "Below_College    0.116\n",
    "Doctor           0.032\n",
    "Name: Education, dtype: float64"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fb83cd77",
   "metadata": {},
   "source": [
    "# Get 30 employees from each Education group\n",
    "attrition_eq = attrition_pop.groupby('Education')\\\n",
    "    .sample(n=30, random_state=2022)\n",
    "\n",
    "\n",
    "# Print the sample\n",
    "print(attrition_eq)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dc62dd43",
   "metadata": {},
   "source": [
    "      Age  Attrition     BusinessTravel  DailyRate            Department  ...  WorkLifeBalance YearsAtCompany YearsInCurrentRole YearsSinceLastPromotion YearsWithCurrManager\n",
    "1191   53        0.0      Travel_Rarely        238                 Sales  ...             Best             14                  7                       8                   10\n",
    "407    29        0.0  Travel_Frequently        995  Research_Development  ...             Best              6                  4                       1                    3\n",
    "1233   59        0.0  Travel_Frequently       1225                 Sales  ...             Good              4                  3                       1                    3\n",
    "366    37        0.0      Travel_Rarely        571  Research_Development  ...             Good              5                  3                       4                    3\n",
    "702    31        0.0  Travel_Frequently        163  Research_Development  ...             Good              5                  4                       1                    4\n",
    "...   ...        ...                ...        ...                   ...  ...              ...            ...                ...                     ...                  ...\n",
    "774    33        0.0      Travel_Rarely        922  Research_Development  ...           Better              6                  1                       0                    5\n",
    "869    45        0.0      Travel_Rarely       1015  Research_Development  ...           Better             10                  7                       1                    4\n",
    "530    32        0.0      Travel_Rarely        120  Research_Development  ...           Better              5                  4                       1                    4\n",
    "1049   48        0.0      Travel_Rarely        163                 Sales  ...           Better              9                  7                       6                    7\n",
    "350    29        1.0      Travel_Rarely        408  Research_Development  ...             Best              2                  2                       1                    1\n",
    "\n",
    "[150 rows x 31 columns]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eb003e0c",
   "metadata": {},
   "source": [
    "# Get 30 employees from each Education group\n",
    "attrition_eq = attrition_pop.groupby('Education')\\\n",
    "\t.sample(n=30, random_state=2022)      \n",
    "\n",
    "# Get the proportions from attrition_eq\n",
    "education_counts_eq = attrition_eq['Education'].value_counts(normalize=True)\n",
    "\n",
    "# Print the results\n",
    "print(education_counts_eq)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3b48d78a",
   "metadata": {},
   "source": [
    "Below_College    0.2\n",
    "College          0.2\n",
    "Bachelor         0.2\n",
    "Master           0.2\n",
    "Doctor           0.2\n",
    "Name: Education, dtype: float64"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5d8d8bb9",
   "metadata": {},
   "source": [
    "# Plot YearsAtCompany from attrition_pop as a histogram\n",
    "attrition_pop['YearsAtCompany'].hist(bins=np.arange(0, 41, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "62c6cf31",
   "metadata": {},
   "source": [
    "# Plot YearsAtCompany from attrition_pop as a histogram\n",
    "attrition_pop['YearsAtCompany'].hist(bins=np.arange(0, 41, 1))\n",
    "plt.show()\n",
    "\n",
    "# Sample 400 employees weighted by YearsAtCompany\n",
    "attrition_weight = attrition_pop.sample(n=400, weights='YearsAtCompany')\n",
    "\n",
    "# Print the sample\n",
    "print(attrition_weight)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d1490066",
   "metadata": {},
   "source": [
    "      Age  Attrition BusinessTravel  DailyRate            Department  ...  WorkLifeBalance YearsAtCompany YearsInCurrentRole YearsSinceLastPromotion YearsWithCurrManager\n",
    "461    29        1.0  Travel_Rarely        318  Research_Development  ...             Good              7                  7                       0                    7\n",
    "533    37        0.0  Travel_Rarely        290  Research_Development  ...             Good              8                  7                       1                    7\n",
    "1155   44        0.0     Non-Travel        489  Research_Development  ...             Best              3                  2                       1                    2\n",
    "669    45        0.0     Non-Travel        805  Research_Development  ...             Good              9                  7                       0                    8\n",
    "1259   42        0.0     Non-Travel        335  Research_Development  ...           Better             20                  9                       3                    7\n",
    "...   ...        ...            ...        ...                   ...  ...              ...            ...                ...                     ...                  ...\n",
    "421    34        0.0  Travel_Rarely        181  Research_Development  ...           Better              5                  0                       1                    2\n",
    "1152   36        0.0  Travel_Rarely       1223  Research_Development  ...           Better             17                 14                      12                    8\n",
    "1151   36        0.0  Travel_Rarely       1299  Research_Development  ...             Good              7                  7                       7                    7\n",
    "1176   36        0.0  Travel_Rarely        363  Research_Development  ...           Better              7                  7                       7                    7\n",
    "700    44        0.0  Travel_Rarely        921  Research_Development  ...           Better              8                  7                       6                    7\n",
    "\n",
    "[400 rows x 31 columns]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b840d911",
   "metadata": {},
   "source": [
    "# Plot YearsAtCompany from attrition_pop as a histogram\n",
    "attrition_pop['YearsAtCompany'].hist(bins=np.arange(0, 41, 1))\n",
    "plt.show()\n",
    "\n",
    "# Sample 400 employees weighted by YearsAtCompany\n",
    "attrition_weight = attrition_pop.sample(n=400, weights=\"YearsAtCompany\")\n",
    "\n",
    "# Plot YearsAtCompany from attrition_weight as a histogram\n",
    "attrition_weight['YearsAtCompany'].hist(bins=np.arange(0, 41, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "65cbf8aa",
   "metadata": {},
   "source": [
    "# Create a list of unique JobRole values\n",
    "job_roles_pop = list(attrition_pop['JobRole'].unique())\n",
    "\n",
    "# Randomly sample four JobRole values\n",
    "job_roles_samp = random.sample(job_roles_pop, k=4)\n",
    "\n",
    "# Print the result\n",
    "print(job_roles_samp)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f07448df",
   "metadata": {},
   "source": [
    "['Research_Director', 'Research_Scientist', 'Human_Resources', 'Manager']"
   ]
  },
  {
   "cell_type": "raw",
   "id": "37b33445",
   "metadata": {},
   "source": [
    "# Create a list of unique JobRole values\n",
    "job_roles_pop = list(attrition_pop['JobRole'].unique())\n",
    "\n",
    "# Randomly sample four JobRole values\n",
    "job_roles_samp = random.sample(job_roles_pop, k=4)\n",
    "\n",
    "# Filter for rows where JobRole is in job_roles_samp\n",
    "jobrole_condition = attrition_pop['JobRole'].isin(job_roles_samp)\n",
    "attrition_filtered = attrition_pop[jobrole_condition]\n",
    "\n",
    "# Print the result\n",
    "print(attrition_filtered)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5d24d9e7",
   "metadata": {},
   "source": [
    "      Age  Attrition BusinessTravel  DailyRate            Department  ...  WorkLifeBalance YearsAtCompany YearsInCurrentRole YearsSinceLastPromotion YearsWithCurrManager\n",
    "0      21        0.0  Travel_Rarely        391  Research_Development  ...           Better              0                  0                       0                    0\n",
    "5      27        0.0     Non-Travel        443  Research_Development  ...             Good              0                  0                       0                    0\n",
    "6      18        0.0     Non-Travel        287  Research_Development  ...           Better              0                  0                       0                    0\n",
    "10     18        0.0     Non-Travel       1431  Research_Development  ...              Bad              0                  0                       0                    0\n",
    "17     31        0.0  Travel_Rarely       1082  Research_Development  ...           Better              1                  1                       1                    0\n",
    "...   ...        ...            ...        ...                   ...  ...              ...            ...                ...                     ...                  ...\n",
    "1462   54        0.0  Travel_Rarely        584  Research_Development  ...           Better             10                  8                       4                    7\n",
    "1464   55        0.0  Travel_Rarely        452  Research_Development  ...           Better             36                 10                       4                   13\n",
    "1465   55        0.0  Travel_Rarely       1117                 Sales  ...           Better             10                  9                       7                    7\n",
    "1466   58        0.0     Non-Travel        350                 Sales  ...             Good             16                  9                      14                   14\n",
    "1469   58        1.0  Travel_Rarely        286  Research_Development  ...           Better             31                 15                      13                    8\n",
    "\n",
    "[526 rows x 31 columns]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "23fdc462",
   "metadata": {},
   "source": [
    "# Create a list of unique JobRole values\n",
    "job_roles_pop = list(attrition_pop['JobRole'].unique())\n",
    "\n",
    "# Randomly sample four JobRole values\n",
    "job_roles_samp = random.sample(job_roles_pop, k=4)\n",
    "\n",
    "# Filter for rows where JobRole is in job_roles_samp\n",
    "jobrole_condition = attrition_pop['JobRole'].isin(job_roles_samp)\n",
    "attrition_filtered = attrition_pop[jobrole_condition]\n",
    "\n",
    "# Remove categories with no rows\n",
    "attrition_filtered['JobRole'] = attrition_filtered['JobRole'].cat.remove_unused_categories()\n",
    "\n",
    "# Randomly sample 10 employees from each sampled job role\n",
    "attrition_clust = attrition_filtered.groupby('JobRole')\\\n",
    "    .sample(n=10, random_state=2022)\n",
    "\n",
    "\n",
    "# Print the sample\n",
    "print(attrition_clust)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d3d09ea8",
   "metadata": {},
   "source": [
    "      Age  Attrition     BusinessTravel  DailyRate            Department  ...  WorkLifeBalance YearsAtCompany YearsInCurrentRole YearsSinceLastPromotion YearsWithCurrManager\n",
    "1348   44        1.0      Travel_Rarely       1376       Human_Resources  ...           Better             20                  6                       3                    6\n",
    "886    41        0.0         Non-Travel        552       Human_Resources  ...           Better              3                  2                       1                    2\n",
    "983    39        0.0      Travel_Rarely        141       Human_Resources  ...              Bad              8                  3                       3                    6\n",
    "88     27        1.0  Travel_Frequently       1337       Human_Resources  ...           Better              1                  0                       0                    0\n",
    "189    34        0.0      Travel_Rarely        829       Human_Resources  ...              Bad              3                  2                       0                    2\n",
    "160    24        0.0  Travel_Frequently        897       Human_Resources  ...           Better              2                  2                       2                    1\n",
    "839    46        0.0      Travel_Rarely        991       Human_Resources  ...             Best              7                  6                       5                    7\n",
    "966    30        0.0      Travel_Rarely       1240       Human_Resources  ...              Bad             11                  9                       4                    7\n",
    "162    28        0.0         Non-Travel        280       Human_Resources  ...           Better              3                  2                       2                    2\n",
    "1231   37        0.0      Travel_Rarely       1239       Human_Resources  ...             Good             10                  0                       4                    7\n",
    "1375   44        0.0      Travel_Rarely       1315  Research_Development  ...             Best              2                  2                       0                    1\n",
    "1462   54        0.0      Travel_Rarely        584  Research_Development  ...           Better             10                  8                       4                    7\n",
    "1316   45        0.0  Travel_Frequently        364  Research_Development  ...           Better              0                  0                       0                    0\n",
    "1356   48        0.0  Travel_Frequently        117  Research_Development  ...           Better             22                 17                       4                    7\n",
    "1387   48        0.0         Non-Travel       1262  Research_Development  ...             Good              5                  4                       2                    1\n",
    "1321   54        0.0         Non-Travel        142       Human_Resources  ...           Better              5                  3                       4                    4\n",
    "1266   50        0.0      Travel_Rarely       1452  Research_Development  ...           Better              5                  4                       4                    4\n",
    "1330   46        0.0      Travel_Rarely        406                 Sales  ...           Better             12                  9                       4                    9\n",
    "1052   59        0.0      Travel_Rarely       1089                 Sales  ...              Bad              6                  4                       0                    4\n",
    "1449   52        0.0      Travel_Rarely        699  Research_Development  ...           Better             33                 18                      11                    9\n",
    "1439   58        0.0      Travel_Rarely       1055  Research_Development  ...           Better              9                  8                       1                    5\n",
    "1339   58        0.0  Travel_Frequently       1216  Research_Development  ...           Better              2                  2                       2                    2\n",
    "1426   49        0.0      Travel_Rarely       1245  Research_Development  ...           Better             31                  9                       0                    9\n",
    "1415   48        0.0      Travel_Rarely       1224  Research_Development  ...           Better             22                 10                      12                    9\n",
    "1322   51        0.0      Travel_Rarely        684  Research_Development  ...           Better             20                 18                      15                   15\n",
    "1284   40        0.0      Travel_Rarely       1308  Research_Development  ...             Best             20                  7                       4                    9\n",
    "1149   37        0.0      Travel_Rarely        161  Research_Development  ...           Better             16                 11                       6                    8\n",
    "1126   42        0.0      Travel_Rarely        810  Research_Development  ...           Better              1                  0                       0                    0\n",
    "1374   46        0.0      Travel_Rarely       1009  Research_Development  ...              Bad              3                  2                       0                    1\n",
    "1050   33        0.0      Travel_Rarely        213  Research_Development  ...             Best             13                  9                       3                    7\n",
    "86     26        0.0      Travel_Rarely        482  Research_Development  ...             Good              1                  0                       1                    0\n",
    "930    52        1.0      Travel_Rarely        723  Research_Development  ...             Good              8                  2                       7                    7\n",
    "860    37        0.0      Travel_Rarely        674  Research_Development  ...           Better             10                  8                       3                    7\n",
    "36     20        1.0      Travel_Rarely       1362  Research_Development  ...           Better              1                  0                       1                    1\n",
    "997    32        0.0      Travel_Rarely        824  Research_Development  ...           Better              7                  1                       2                    5\n",
    "1358   45        0.0      Travel_Rarely       1339  Research_Development  ...           Better              1                  0                       0                    0\n",
    "993    41        0.0  Travel_Frequently       1200  Research_Development  ...             Good              6                  2                       3                    3\n",
    "421    34        0.0      Travel_Rarely        181  Research_Development  ...           Better              5                  0                       1                    2\n",
    "789    28        1.0      Travel_Rarely        654  Research_Development  ...           Better              7                  7                       3                    7\n",
    "94     36        1.0      Travel_Rarely        318  Research_Development  ...             Good              1                  0                       0                    0\n",
    "\n",
    "[40 rows x 31 columns]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a45d5e32",
   "metadata": {},
   "source": [
    "# Perform simple random sampling to get 0.25 of the population\n",
    "attrition_srs = attrition_pop.sample(frac=0.25, random_state=2022)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "300b30c1",
   "metadata": {},
   "source": [
    "# Perform stratified sampling to get 0.25 of each relationship group\n",
    "attrition_strat = attrition_pop.groupby('RelationshipSatisfaction')\\\n",
    "    .sample(frac=0.25, random_state=2022)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cb090267",
   "metadata": {},
   "source": [
    "# Create a list of unique RelationshipSatisfaction values\n",
    "satisfaction_unique = list(attrition_pop['RelationshipSatisfaction'].unique())\n",
    "\n",
    "# Randomly sample 2 unique satisfaction values\n",
    "satisfaction_samp = random.sample(satisfaction_unique, k=2)\n",
    "\n",
    "# Filter for satisfaction_samp and clear unused categories from RelationshipSatisfaction\n",
    "satis_condition = attrition_pop['RelationshipSatisfaction'].isin(satisfaction_samp)\n",
    "attrition_clust_prep = attrition_pop[satis_condition]\n",
    "attrition_clust_prep['RelationshipSatisfaction'] = attrition_clust_prep['RelationshipSatisfaction']\\\n",
    "    .cat.remove_unused_categories()\n",
    "\n",
    "# Perform cluster sampling on the selected group, getting 0.25 of attrition_pop\n",
    "attrition_clust = attrition_clust_prep.groupby('RelationshipSatisfaction')\\\n",
    "    .sample(n=len(attrition_pop)// 4, random_state=2022)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "639a7c2d",
   "metadata": {},
   "source": [
    "# Mean Attrition by RelationshipSatisfaction group\n",
    "mean_attrition_pop = attrition_pop.groupby('RelationshipSatisfaction')\\\n",
    "    ['Attrition'].mean()\n",
    "\n",
    "# Print the result\n",
    "print(mean_attrition_pop)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5c68b5f9",
   "metadata": {},
   "source": [
    "RelationshipSatisfaction\n",
    "Low          0.207\n",
    "Medium       0.149\n",
    "High         0.155\n",
    "Very_High    0.148\n",
    "Name: Attrition, dtype: float64"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1611e73d",
   "metadata": {},
   "source": [
    "# Calculate the same thing for the simple random sample \n",
    "mean_attrition_srs = attrition_srs.groupby('RelationshipSatisfaction')\\\n",
    "    ['Attrition'].mean()\n",
    "\n",
    "# Print the result\n",
    "print(mean_attrition_srs)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e6367e8",
   "metadata": {},
   "source": [
    "RelationshipSatisfaction\n",
    "Low          0.134\n",
    "Medium       0.164\n",
    "High         0.160\n",
    "Very_High    0.156\n",
    "Name: Attrition, dtype: float64"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8b594454",
   "metadata": {},
   "source": [
    "# Calculate the same thing for the stratified sample \n",
    "mean_attrition_strat = attrition_strat.groupby('RelationshipSatisfaction')\\\n",
    "    ['Attrition'].mean()\n",
    "\n",
    "# Print the result\n",
    "print(mean_attrition_strat)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "74cb2d0a",
   "metadata": {},
   "source": [
    "RelationshipSatisfaction\n",
    "Low          0.145\n",
    "Medium       0.079\n",
    "High         0.165\n",
    "Very_High    0.130\n",
    "Name: Attrition, dtype: float64"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b5011419",
   "metadata": {},
   "source": [
    "# Calculate the same thing for the cluster sample \n",
    "mean_attrition_clust = attrition_clust.groupby('RelationshipSatisfaction')\\\n",
    "    ['Attrition'].mean()\n",
    "\n",
    "# Print the result\n",
    "print(mean_attrition_clust)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ef216605",
   "metadata": {},
   "source": [
    "<script.py> output:\n",
    "    RelationshipSatisfaction\n",
    "    Low          0.145\n",
    "    Medium       0.079\n",
    "    High         0.165\n",
    "    Very_High    0.130\n",
    "    Name: Attrition, dtype: float64"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0e34d742",
   "metadata": {},
   "source": [
    "# Generate a simple random sample of 50 rows, with seed 2022\n",
    "attrition_srs50 = attrition_pop.sample(n=50, random_state=2022)\n",
    "\n",
    "# Calculate the mean employee attrition in the sample\n",
    "mean_attrition_srs50 = attrition_srs50['Attrition'].mean()\n",
    "\n",
    "# Calculate the relative error percentage\n",
    "rel_error_pct50 = (abs((mean_attrition_pop - mean_attrition_srs50))/(mean_attrition_pop))*100\n",
    "\n",
    "# Print rel_error_pct50\n",
    "print(rel_error_pct50)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a536130",
   "metadata": {},
   "source": [
    "62.78481012658228"
   ]
  },
  {
   "cell_type": "raw",
   "id": "46e6e564",
   "metadata": {},
   "source": [
    "# Generate a simple random sample of 100 rows, with seed 2022\n",
    "attrition_srs100 = attrition_pop.sample(n=100, random_state=2022)\n",
    "\n",
    "\n",
    "# Calculate the mean employee attrition in the sample\n",
    "mean_attrition_srs100 = attrition_srs100['Attrition'].mean()\n",
    "\n",
    "# Calculate the relative error percentage\n",
    "rel_error_pct100 = (abs(mean_attrition_pop - mean_attrition_srs100)/mean_attrition_pop)*100\n",
    "\n",
    "# Print rel_error_pct100\n",
    "print(rel_error_pct100)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a80b22f5",
   "metadata": {},
   "source": [
    "6.962025316455694"
   ]
  },
  {
   "cell_type": "raw",
   "id": "34631b3a",
   "metadata": {},
   "source": [
    "# Create an empty list\n",
    "mean_attritions = []\n",
    "# Loop 500 times to create 500 sample means\n",
    "for i in range(500):\n",
    "\tmean_attritions.append(\n",
    "    \tattrition_pop.sample(n=60)['Attrition'].mean()\n",
    "\t)\n",
    "  \n",
    "# Print out the first few entries of the list\n",
    "print(mean_attritions[0:5])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "197ff578",
   "metadata": {},
   "source": [
    "[0.23333333333333334, 0.13333333333333333, 0.18333333333333332, 0.16666666666666666, 0.1]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a52c8e6c",
   "metadata": {},
   "source": [
    "# Create an empty list\n",
    "mean_attritions = []\n",
    "# Loop 500 times to create 500 sample means\n",
    "for i in range(500):\n",
    "\tmean_attritions.append(\n",
    "    \tattrition_pop.sample(n=60)['Attrition'].mean()\n",
    "\t)\n",
    "\n",
    "# Create a histogram of the 500 sample means\n",
    "plt.hist(mean_attritions, bins=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3a5659b3",
   "metadata": {},
   "source": [
    "# Expand a grid representing 5 8-sided dice\n",
    "dice = expand_grid(\n",
    "  {'die1': [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "   'die2': [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "   'die3': [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "   'die4': [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "   'die5': [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "  })\n",
    "\n",
    "# Print the result\n",
    "print(dice)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6d1ba5b4",
   "metadata": {},
   "source": [
    "       die1  die2  die3  die4  die5\n",
    "0         1     1     1     1     1\n",
    "1         1     1     1     1     2\n",
    "2         1     1     1     1     3\n",
    "3         1     1     1     1     4\n",
    "4         1     1     1     1     5\n",
    "...     ...   ...   ...   ...   ...\n",
    "32763     8     8     8     8     4\n",
    "32764     8     8     8     8     5\n",
    "32765     8     8     8     8     6\n",
    "32766     8     8     8     8     7\n",
    "32767     8     8     8     8     8\n",
    "\n",
    "[32768 rows x 5 columns]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a58838ec",
   "metadata": {},
   "source": [
    "# Expand a grid representing 5 8-sided dice\n",
    "dice = expand_grid(\n",
    "  {'die1': [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "   'die2': [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "   'die3': [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "   'die4': [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "   'die5': [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "  })\n",
    "\n",
    "# Add a column of mean rolls and convert to a categorical\n",
    "dice['mean_roll'] = (dice['die1'] + dice['die2'] + dice['die3'] + dice['die4'] + dice['die5'])/5\n",
    "                     \n",
    "                    \n",
    "dice['mean_roll'] = dice['mean_roll'].astype('category')\n",
    "\n",
    "# Print result\n",
    "print(dice)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5e76aa12",
   "metadata": {},
   "source": [
    "       die1  die2  die3  die4  die5 mean_roll\n",
    "0         1     1     1     1     1       1.0\n",
    "1         1     1     1     1     2       1.2\n",
    "2         1     1     1     1     3       1.4\n",
    "3         1     1     1     1     4       1.6\n",
    "4         1     1     1     1     5       1.8\n",
    "...     ...   ...   ...   ...   ...       ...\n",
    "32763     8     8     8     8     4       7.2\n",
    "32764     8     8     8     8     5       7.4\n",
    "32765     8     8     8     8     6       7.6\n",
    "32766     8     8     8     8     7       7.8\n",
    "32767     8     8     8     8     8       8.0\n",
    "\n",
    "[32768 rows x 6 columns]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "de132f10",
   "metadata": {},
   "source": [
    "# Expand a grid representing 5 8-sided dice\n",
    "dice = expand_grid(\n",
    "  {'die1': [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "   'die2': [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "   'die3': [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "   'die4': [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "   'die5': [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "  })\n",
    "\n",
    "# Add a column of mean rolls and convert to a categorical\n",
    "dice['mean_roll'] = (dice['die1'] + dice['die2'] + \n",
    "                     dice['die3'] + dice['die4'] + \n",
    "                     dice['die5']) / 5\n",
    "dice['mean_roll'] = dice['mean_roll'].astype('category')\n",
    "\n",
    "# Draw a bar plot of mean_roll\n",
    "dice['mean_roll'].value_counts(sort=False).plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "485a37e4",
   "metadata": {},
   "source": [
    "# Sample one to eight, five times, with replacement\n",
    "five_rolls = np.random.choice(list(range(1,9)), size=5, replace=True)\n",
    "\n",
    "# Print the mean of five_rolls\n",
    "print(five_rolls.mean())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ccdfcdc9",
   "metadata": {},
   "source": [
    "5.6"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bcf497ec",
   "metadata": {},
   "source": [
    "# Replicate the sampling code 1000 times\n",
    "sample_means_1000 = []\n",
    "for i in range(1000):\n",
    "    sample_means_1000.append(\n",
    "    np.random.choice(list(range(1, 9)), size=5, replace=True).mean()\n",
    "    )\n",
    "    \n",
    "# Print the first 10 entries of the result\n",
    "print(sample_means_1000[0:10])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7a1e6890",
   "metadata": {},
   "source": [
    "[5.6, 3.8, 3.2, 4.0, 4.4, 4.0, 4.4, 5.8, 3.6, 5.2]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "128f18bc",
   "metadata": {},
   "source": [
    "# Replicate the sampling code 1000 times\n",
    "sample_means_1000 = []\n",
    "for i in range(1000):\n",
    "    sample_means_1000.append(\n",
    "  \t\tnp.random.choice(list(range(1, 9)), size=5, replace=True).mean()\n",
    "    )\n",
    "\n",
    "# Draw a histogram of sample_means_1000 with 20 bins\n",
    "plt.hist(sample_means_1000,bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "47ee38f5",
   "metadata": {},
   "source": [
    "# Calculate the mean of the mean attritions for each sampling distribution\n",
    "mean_of_means_5 = np.mean(sampling_distribution_5)\n",
    "mean_of_means_50 = np.mean(sampling_distribution_50)\n",
    "mean_of_means_500 =  np.mean(sampling_distribution_500)\n",
    "\n",
    "# Print the results\n",
    "print(mean_of_means_5)\n",
    "print(mean_of_means_50)\n",
    "print(mean_of_means_500)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "87fdbfeb",
   "metadata": {},
   "source": [
    "0.16560000000000002\n",
    "0.16416000000000003\n",
    "0.16119799999999998"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f8e2e946",
   "metadata": {},
   "source": [
    "# Calculate the std. dev. of the mean attritions for each sampling distribution\n",
    "sd_of_means_5 = np.std(sampling_distribution_5, ddof=1)\n",
    "sd_of_means_50 = np.std(sampling_distribution_50, ddof=1)\n",
    "sd_of_means_500 = np.std(sampling_distribution_500, ddof=1)\n",
    "\n",
    "# Print the results\n",
    "print(sd_of_means_5)\n",
    "print(sd_of_means_50)\n",
    "print(sd_of_means_500)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "09777d6a",
   "metadata": {},
   "source": [
    "0.16602470961939433\n",
    "0.05050589317342329\n",
    "0.013451458572873713"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5d0f3e5e",
   "metadata": {},
   "source": [
    "# Generate 1 bootstrap resample\n",
    "spotify_1_resample = spotify_sample.sample(frac=1,replace=True)\n",
    "\n",
    "# Print the resample\n",
    "print(spotify_1_resample)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ec6ad36a",
   "metadata": {},
   "source": [
    "       index                                            artists                                     name  danceability\n",
    "15206  15206                                    ['Chris Cagle']                        Anywhere But Here         0.640\n",
    "7230    7230                                     ['White Lies']                                    Death         0.454\n",
    "21070  21070                                       ['Chevelle']                                  The Red         0.509\n",
    "1704    1704                                     ['amazarashi']                           Sora Ni Utaeba         0.594\n",
    "12039  12039                         ['NF', 'Jeremiah Carlson']                             I'll Keep On         0.678\n",
    "...      ...                                                ...                                      ...           ...\n",
    "32722  32722                                   ['Chase Bryant']                        Little Bit of You         0.612\n",
    "7909    7909                                     ['Jackie Lee']                         Getting Over You         0.446\n",
    "40546  40546                                 ['Kelly Clarkson']           My Life Would Suck Without You         0.526\n",
    "26946  26946  ['Rae Sremmurd', 'Swae Lee', 'Slim Jxmmi', 'Ju...  Powerglide (feat. Juicy J) - From SR3MM         0.713\n",
    "39836  39836                                       ['Keahiwai']                                  Falling         0.659\n",
    "\n",
    "[41656 rows x 4 columns]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "df915285",
   "metadata": {},
   "source": [
    "# Generate 1 bootstrap resample\n",
    "spotify_1_resample = spotify_sample.sample(frac=1, replace=True)\n",
    "\n",
    "# Calculate of the danceability column of spotify_1_resample\n",
    "mean_danceability_1 = np.mean(spotify_1_resample['danceability'])\n",
    "\n",
    "# Print the result\n",
    "print(mean_danceability_1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8e1ccaa3",
   "metadata": {},
   "source": [
    "0.5910226305934319"
   ]
  },
  {
   "cell_type": "raw",
   "id": "10cbf829",
   "metadata": {},
   "source": [
    "# Replicate this 1000 times\n",
    "mean_danceability_1000 = []\n",
    "for i in range(1000):\n",
    " mean_danceability_1000.append(\n",
    "        np.mean(spotify_sample.sample(frac=1, replace=True)['danceability'])\n",
    " )\n",
    "  \n",
    "# Print the result\n",
    "print(mean_danceability_1000)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4000ab47",
   "metadata": {},
   "source": [
    "# Replicate this 1000 times\n",
    "mean_danceability_1000 = []\n",
    "for i in range(1000):\n",
    "\tmean_danceability_1000.append(\n",
    "        np.mean(spotify_sample.sample(frac=1, replace=True)['danceability'])\n",
    "\t)\n",
    "\n",
    "# Draw a histogram of the resample means\n",
    "plt.hist(mean_danceability_1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2ba9e942",
   "metadata": {},
   "source": [
    "mean_popularity_2000_samp = []\n",
    "\n",
    "# Generate a sampling distribution of 2000 replicates\n",
    "for _ in range(2000):\n",
    "    # Sample 500 rows from the population without replacement\n",
    "    sample = spotify_population.sample(n=500, replace=False)\n",
    "    \n",
    "    # Calculate the mean popularity score of the sample\n",
    "    mean_popularity = np.mean(sample['popularity'])\n",
    "    \n",
    "    # Append the mean popularity score to the list\n",
    "    mean_popularity_2000_samp.append(mean_popularity)\n",
    "\n",
    "# Print the sampling distribution results\n",
    "print(mean_popularity_2000_samp)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ca47ca51",
   "metadata": {},
   "source": [
    "mean_popularity_2000_boot = []\n",
    "\n",
    "# Generate a bootstrap distribution of 2000 replicates\n",
    "for _ in range(2000):\n",
    "    # Resample 500 rows from the sample with replacement\n",
    "    bootstrap_sample = spotify_sample.sample(n=500, replace=True)\n",
    "    \n",
    "    # Calculate the mean popularity score of the bootstrap sample\n",
    "    mean_popularity = np.mean(bootstrap_sample['popularity'])\n",
    "    \n",
    "    # Append the mean popularity score to the list\n",
    "    mean_popularity_2000_boot.append(mean_popularity)\n",
    "\n",
    "# Print the bootstrap distribution results\n",
    "print(mean_popularity_2000_boot)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "40d863e6",
   "metadata": {},
   "source": [
    "# Calculate the population mean popularity\n",
    "pop_mean = spotify_population['popularity'].mean()\n",
    "\n",
    "# Calculate the original sample mean popularity\n",
    "samp_mean = spotify_sample['popularity'].mean()\n",
    "\n",
    "# Calculate the sampling dist'n estimate of mean popularity\n",
    "samp_distn_mean = np.mean(sampling_distribution)\n",
    "\n",
    "# Calculate the bootstrap dist'n estimate of mean popularity\n",
    "boot_distn_mean = np.mean(bootstrap_distribution)\n",
    "\n",
    "# Print the means\n",
    "print([pop_mean, samp_mean, samp_distn_mean, boot_distn_mean])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "73da7432",
   "metadata": {},
   "source": [
    "[54.837142308430955, 54.8686, 54.83586444000001, 54.86607339999999]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5375038e",
   "metadata": {},
   "source": [
    "# Calculate the population std dev popularity\n",
    "pop_sd = spotify_population['popularity'].std(ddof=0)\n",
    "\n",
    "# Calculate the original sample std dev popularity\n",
    "samp_sd = spotify_sample['popularity'].std()\n",
    "\n",
    "# Calculate the sampling dist'n estimate of std dev popularity\n",
    "samp_distn_sd = np.std(sampling_distribution, ddof=1) * np.sqrt(5000)\n",
    "\n",
    "# Calculate the bootstrap dist'n estimate of std dev popularity\n",
    "boot_distn_sd = np.std(bootstrap_distribution, ddof=1) * np.sqrt(5000)\n",
    "\n",
    "# Print the standard deviations\n",
    "print([pop_sd, samp_sd, samp_distn_sd, boot_distn_sd])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a8333191",
   "metadata": {},
   "source": [
    "[10.880065274257536, 10.85755549570291, 10.13269956909527, 10.874815536126995]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "18023e3c",
   "metadata": {},
   "source": [
    "# Generate a 95% confidence interval using the quantile method\n",
    "lower_quant = np.quantile(bootstrap_distribution,0.025)\n",
    "upper_quant = np.quantile(bootstrap_distribution,0.975)\n",
    "\n",
    "# Print quantile method confidence interval\n",
    "print((lower_quant, upper_quant))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9892b0f1",
   "metadata": {},
   "source": [
    "(54.56816, 55.1674)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1244eb31",
   "metadata": {},
   "source": [
    "# Find the mean and std dev of the bootstrap distribution\n",
    "point_estimate = np.mean(bootstrap_distribution)\n",
    "standard_error = np.std(bootstrap_distribution,ddof=1)\n",
    "\n",
    "# Find the lower limit of the confidence interval\n",
    "lower_se = norm.ppf(0.025,loc=point_estimate,scale=standard_error)\n",
    "\n",
    "# Find the upper limit of the confidence interval\n",
    "upper_se = norm.ppf(0.975,loc=point_estimate,scale=standard_error)\n",
    "\n",
    "# Print standard error method confidence interval\n",
    "print((lower_se, upper_se))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "64086d26",
   "metadata": {},
   "source": [
    "(54.56464443118767, 55.16750236881231)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5519ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
